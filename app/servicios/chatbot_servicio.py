"""
Servicio de Chatbot Inteligente - VersiÃ³n Mejorada
Implementa Groq + OpenAI fallback, prompts multinivel, logs conversacionales, 
copiloto adaptativo, cachÃ© inteligente, memoria conversacional y validaciÃ³n avanzada
"""

import os
import json
import logging
import hashlib
import html
import re
from datetime import datetime, timedelta
from typing import Dict, List, Optional, Any
from functools import lru_cache
import traceback

# AI Providers
try:
    from groq import Groq
    GROQ_AVAILABLE = True
except ImportError:
    GROQ_AVAILABLE = False
    Groq = None

try:
    import openai
    OPENAI_AVAILABLE = True
except ImportError:
    OPENAI_AVAILABLE = False
    openai = None

# Imports relativos - solo cuando se ejecuta como parte de un paquete
if __name__ != "__main__":
    from ..utils.base_datos import get_db_connection
    from .chatbot_prompts import ChatbotPrompts
    from .niveles import SistemaNiveles

# Configure logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class ChatbotServicio:
    """
    Servicio principal del chatbot con funcionalidades avanzadas:
    - Groq + OpenAI fallback con retry automÃ¡tico
    - Prompts multinivel (bÃ¡sico/intermedio/expert)
    - Logs conversacionales
    - Copiloto adaptativo
    - Sistema de cachÃ© inteligente
    - Memoria conversacional mejorada
    - ValidaciÃ³n y sanitizaciÃ³n de mensajes
    - Analytics y mÃ©tricas
    """

    def __init__(self):
        self.groq_client = None
        self.openai_client = None
        self.response_cache = {}  # Cache de respuestas con TTL
        self.cache_ttl = timedelta(hours=1)  # Tiempo de vida del cachÃ©
        self._initialize_clients()

    def _initialize_clients(self):
        """Inicializar clientes de IA con fallback"""
        # Initialize Groq
        if GROQ_AVAILABLE:
            groq_key = os.getenv("GROQ_API_KEY", "")
            if groq_key:
                try:
                    self.groq_client = Groq(api_key=groq_key)
                    logger.info("âœ… Groq client inicializado")
                except Exception as e:
                    logger.error(f"âŒ Error inicializando Groq: {e}")
                    self.groq_client = None

        # Initialize OpenAI
        if OPENAI_AVAILABLE:
            openai_key = os.getenv("OPENAI_API_KEY", "")
            if openai_key:
                try:
                    self.openai_client = openai.OpenAI(api_key=openai_key)
                    logger.info("âœ… OpenAI client inicializado")
                except Exception as e:
                    logger.error(f"âŒ Error inicializando OpenAI: {e}")
                    self.openai_client = None

        if not self.groq_client and not self.openai_client:
            logger.warning("âš ï¸ NingÃºn cliente de IA disponible - usando modo fallback")
    
    def validar_mensaje(self, mensaje: str) -> Dict[str, Any]:
        """
        Valida y sanitiza el mensaje del usuario
        """
        if not mensaje or len(mensaje.strip()) == 0:
            return {"valid": False, "error": "El mensaje no puede estar vacÃ­o"}
        
        if len(mensaje) > 2000:
            return {"valid": False, "error": "El mensaje es demasiado largo (mÃ¡ximo 2000 caracteres)"}
        
        # Detectar contenido inapropiado bÃ¡sico
        palabras_prohibidas = ['spam', 'hack', 'exploit', 'virus', 'malware']
        mensaje_lower = mensaje.lower()
        if any(palabra in mensaje_lower for palabra in palabras_prohibidas):
            return {"valid": False, "error": "El mensaje contiene contenido no permitido"}
        
        # Sanitizar HTML
        mensaje_sanitizado = html.escape(mensaje)
        
        return {
            "valid": True,
            "sanitized": mensaje_sanitizado,
            "original_length": len(mensaje)
        }
    
    def _get_cache_key(self, mensaje: str, nivel: str, analysis_context: Dict = None) -> str:
        """Genera clave Ãºnica para cachÃ©"""
        context_str = json.dumps(analysis_context, sort_keys=True) if analysis_context else ""
        key_string = f"{mensaje}_{nivel}_{context_str}"
        return hashlib.md5(key_string.encode()).hexdigest()
    
    def _get_cached_response(self, cache_key: str) -> Optional[Dict[str, Any]]:
        """Obtiene respuesta del cachÃ© si existe y no ha expirado"""
        if cache_key in self.response_cache:
            cached_data = self.response_cache[cache_key]
            cache_time = datetime.fromisoformat(cached_data.get('cache_timestamp', ''))
            
            if datetime.now() - cache_time < self.cache_ttl:
                logger.info("âœ… Respuesta obtenida de cachÃ©")
                return cached_data
            else:
                # Eliminar cachÃ© expirado
                del self.response_cache[cache_key]
        
        return None
    
    def _save_to_cache(self, cache_key: str, response_data: Dict[str, Any]):
        """Guarda respuesta en cachÃ©"""
        response_data['cache_timestamp'] = datetime.now().isoformat()
        self.response_cache[cache_key] = response_data
        
        # Limpiar cachÃ© antiguo periÃ³dicamente (mantener solo Ãºltimos 100)
        if len(self.response_cache) > 100:
            # Eliminar entradas mÃ¡s antiguas
            sorted_cache = sorted(
                self.response_cache.items(),
                key=lambda x: x[1].get('cache_timestamp', ''),
                reverse=True
            )
            self.response_cache = dict(sorted_cache[:100])
    
    def obtener_historial_conversacion(self, usuario_id: int, limit: int = 5) -> List[Dict]:
        """
        Obtiene historial reciente de conversaciÃ³n para contexto
        """
        if not usuario_id:
            return []
        
        try:
            db = get_db_connection()
            cursor = db.cur
            
            cursor.execute("""
                SELECT mensaje_usuario, respuesta_ia, fecha
                FROM Conversaciones_Chatbot
                WHERE usuario_id = ?
                ORDER BY fecha DESC
                LIMIT ?
            """, (usuario_id, limit))
            
            historial = []
            for row in cursor.fetchall():
                historial.append({
                    'usuario': row[0],
                    'bot': row[1][:300] if row[1] else '',  # Limitar longitud
                    'fecha': row[2]
                })
            
            return list(reversed(historial))  # Orden cronolÃ³gico
        except Exception as e:
            logger.error(f"Error obteniendo historial: {e}")
            return []

    def determinar_nivel_usuario(self, usuario_id: Optional[int] = None,
                               historial_conversaciones: List[Dict] = None) -> str:
        """
        Determina el nivel de expertise del usuario basado en su historial
        Utiliza el sistema de niveles centralizado
        """
        return SistemaNiveles.determinar_nivel_usuario(usuario_id, historial_conversaciones)

    def obtener_prompt_por_nivel(self, nivel: str, contexto: Dict = None, analysis_context: Dict = None) -> str:
        """
        Retorna el prompt apropiado segÃºn el nivel del usuario usando el sistema de prompts mejorado
        """
        # Obtener prompt base del sistema mejorado
        prompt = ChatbotPrompts.BASE_PROMPTS.get(nivel, ChatbotPrompts.BASE_PROMPTS["basico"])

        # Agregar contexto de anÃ¡lisis especÃ­fico usando el sistema mejorado
        if analysis_context and isinstance(analysis_context, dict):
            tipo_analisis = analysis_context.get('tipo_analisis')
            resultados = analysis_context.get('resultados', {})
            
            # Usar el sistema de prompts contextual mejorado
            if tipo_analisis and resultados:
                contextual_prompt = ChatbotPrompts.get_contextual_prompt(tipo_analisis, resultados)
                if contextual_prompt:
                    prompt += "\n\n" + contextual_prompt
                
                # Agregar informaciÃ³n adicional de resultados para contexto
                prompt += f"\n\nRESULTADOS ESPECÃFICOS DEL ANÃLISIS:"
                for key, value in resultados.items():
                    if value is not None and key not in ['escenarios_originales']:
                        if isinstance(value, (int, float)):
                            if 'porcentaje' in key.lower() or 'tir' in key.lower() or 'wacc' in key.lower() or 'rendimiento' in key.lower() or 'riesgo' in key.lower():
                                prompt += f"\n- {key}: {value}%"
                            elif 'probabilidad' in key.lower():
                                prompt += f"\n- {key}: {(value * 100):.1f}%"
                            else:
                                prompt += f"\n- {key}: S/ {value:,.2f}" if value > 1000 else f"\n- {key}: {value}"
                        elif isinstance(value, str):
                            prompt += f"\n- {key}: {value}"

        # Agregar contexto especÃ­fico legacy si estÃ¡ disponible
        elif contexto and isinstance(contexto, dict):
            sim_type = contexto.get('type')
            if sim_type:
                prompt += f"\n\nCONTEXTO ACTUAL: El usuario estÃ¡ trabajando en un anÃ¡lisis de {sim_type}."
                if contexto.get('van'):
                    prompt += f" VAN calculado: {contexto['van']}"
                if contexto.get('tir'):
                    prompt += f" TIR calculada: {contexto['tir']}"

        return prompt

    def consultar_ia(self, mensaje: str, usuario_id: Optional[int] = None,
                    contexto: Dict = None, analysis_context: Dict = None) -> Dict[str, Any]:
        """
        MÃ©todo principal para consultar IA con fallback y funcionalidades avanzadas
        """
        try:
            start_time = datetime.now()
            
            # Validar mensaje
            validacion = self.validar_mensaje(mensaje)
            if not validacion["valid"]:
                return {
                    "success": False,
                    "error": validacion["error"],
                    "respuesta": f"Lo siento, {validacion['error'].lower()}"
                }
            
            mensaje = validacion["sanitized"]
            
            # Debug: Log analysis_context
            logger.info(f"ğŸ” Analysis context received: {analysis_context}")

            # Determinar nivel del usuario
            nivel_usuario = self.determinar_nivel_usuario(usuario_id)

            # Verificar cachÃ©
            cache_key = self._get_cache_key(mensaje, nivel_usuario, analysis_context)
            cached_response = self._get_cached_response(cache_key)
            if cached_response:
                return cached_response

            # Obtener historial de conversaciÃ³n para contexto
            historial_contexto = ""
            if usuario_id:
                historial = self.obtener_historial_conversacion(usuario_id, limit=3)
                if historial:
                    historial_contexto = "\n\nCONTEXTO DE CONVERSACIÃ“N RECIENTE:\n"
                    for i, conv in enumerate(historial, 1):
                        historial_contexto += f"{i}. Usuario: {conv['usuario'][:150]}...\n"
                        historial_contexto += f"   Bot: {conv['bot'][:150]}...\n\n"

            # Obtener prompt apropiado con contexto de anÃ¡lisis
            system_prompt = self.obtener_prompt_por_nivel(nivel_usuario, contexto, analysis_context)
            
            # Agregar historial al prompt
            if historial_contexto:
                system_prompt += historial_contexto

            # Intentar primero con Groq
            respuesta = None
            proveedor_usado = None

            if self.groq_client:
                try:
                    respuesta = self._consultar_groq(mensaje, system_prompt)
                    proveedor_usado = "groq"
                    logger.info("âœ… Respuesta obtenida de Groq")
                except Exception as e:
                    logger.warning(f"âŒ Error con Groq, intentando OpenAI: {e}")

            # Fallback a OpenAI si Groq falla
            if not respuesta and self.openai_client:
                try:
                    respuesta = self._consultar_openai(mensaje, system_prompt)
                    proveedor_usado = "openai"
                    logger.info("âœ… Respuesta obtenida de OpenAI (fallback)")
                except Exception as e:
                    logger.error(f"âŒ Error con OpenAI: {e}")

            # Ãšltimo fallback a respuestas predefinidas
            if not respuesta:
                respuesta = self._respuesta_fallback(mensaje, contexto, analysis_context)
                proveedor_usado = "fallback"
                logger.info("âš ï¸ Usando respuesta fallback")

            # Agregar funcionalidades del copiloto adaptativo
            respuesta = self._aplicar_copiloto_adaptativo(respuesta, contexto, nivel_usuario, analysis_context)

            # Aplicar respuestas especÃ­ficas para consultas sobre valores calculados
            respuesta = self._aplicar_respuestas_especificas(respuesta, analysis_context, mensaje)

            # Override completo para preguntas sobre TIR cuando hay contexto de anÃ¡lisis
            respuesta = self._override_respuestas_tir(respuesta, analysis_context, mensaje)

            # Overrides para otros tipos de anÃ¡lisis
            respuesta = self._override_respuestas_van(respuesta, analysis_context, mensaje)
            respuesta = self._override_respuestas_wacc(respuesta, analysis_context, mensaje)
            respuesta = self._override_respuestas_portafolio(respuesta, analysis_context, mensaje)
            respuesta = self._override_respuestas_ml(respuesta, analysis_context, mensaje)

            # Aplicar formato automÃ¡tico de colores y preguntas sugeridas
            respuesta = self._aplicar_formato_automatico(respuesta, contexto, analysis_context, nivel_usuario)

            # Calcular tiempo de respuesta
            response_time = (datetime.now() - start_time).total_seconds()
            
            # Preparar respuesta
            result = {
                "success": True,
                "respuesta": respuesta,
                "proveedor": proveedor_usado,
                "nivel_usuario": nivel_usuario,
                "timestamp": datetime.now().isoformat(),
                "response_time": response_time,
                "cached": False
            }
            
            # Guardar en cachÃ©
            self._save_to_cache(cache_key, result.copy())

            # Log de la conversaciÃ³n (asÃ­ncrono para no bloquear)
            try:
                self._log_conversacion(usuario_id, mensaje, respuesta, proveedor_usado, contexto, nivel_usuario, analysis_context)
            except Exception as e:
                logger.error(f"Error logging conversaciÃ³n (no crÃ­tico): {e}")

            return result

        except Exception as e:
            logger.error(f"âŒ Error en consultar_ia: {e}")
            return {
                "success": False,
                "error": str(e),
                "respuesta": "Lo siento, hubo un error al procesar tu consulta. Por favor, intÃ©ntalo de nuevo."
            }

    def _consultar_groq(self, mensaje: str, system_prompt: str, max_retries: int = 3) -> str:
        """Consulta a Groq API con retry automÃ¡tico"""
        for attempt in range(max_retries):
            try:
                response = self.groq_client.chat.completions.create(
                    model="llama-3.1-8b-instant",
                    messages=[
                        {"role": "system", "content": system_prompt},
                        {"role": "user", "content": mensaje}
                    ],
                    max_tokens=2000,
                    temperature=0.7,
                )
                return response.choices[0].message.content.strip()
            except Exception as e:
                if attempt < max_retries - 1:
                    logger.warning(f"Intento {attempt + 1} fallido con Groq, reintentando...: {e}")
                    import time
                    time.sleep(1 * (attempt + 1))  # Backoff exponencial
                else:
                    raise e

    def _consultar_openai(self, mensaje: str, system_prompt: str, max_retries: int = 3) -> str:
        """Consulta a OpenAI API con retry automÃ¡tico"""
        for attempt in range(max_retries):
            try:
                response = self.openai_client.chat.completions.create(
                    model="gpt-3.5-turbo",
                    messages=[
                        {"role": "system", "content": system_prompt},
                        {"role": "user", "content": mensaje}
                    ],
                    max_tokens=2000,
                    temperature=0.7,
                )
                return response.choices[0].message.content.strip()
            except Exception as e:
                if attempt < max_retries - 1:
                    logger.warning(f"Intento {attempt + 1} fallido con OpenAI, reintentando...: {e}")
                    import time
                    time.sleep(1 * (attempt + 1))  # Backoff exponencial
                else:
                    raise e

    def _respuesta_fallback(self, mensaje: str, contexto: Dict = None, analysis_context: Dict = None) -> str:
        """Respuestas predefinidas cuando IA no estÃ¡ disponible"""
        mensaje_lower = mensaje.lower()

        # Respuestas bÃ¡sicas
        if "van" in mensaje_lower and ("que es" in mensaje_lower or "quÃ© es" in mensaje_lower):
            return "**Â¿QuÃ© es el VAN?**\n\nEl VAN (Valor Actual Neto) mide la rentabilidad real de una inversiÃ³n, considerando el tiempo y el riesgo del dinero.\n\n**FÃ³rmula bÃ¡sica:** VAN = Flujos de caja descontados - InversiÃ³n inicial\n\n**InterpretaciÃ³n:**\nâ€¢ VAN > 0: InversiÃ³n rentable\nâ€¢ VAN < 0: InversiÃ³n no rentable\nâ€¢ VAN = 0: Punto de equilibrio"

        if "tir" in mensaje_lower and ("que es" in mensaje_lower or "quÃ© es" in mensaje_lower):
            return "**Â¿QuÃ© es la TIR?**\n\nLa TIR (Tasa Interna de Retorno) es el porcentaje de ganancia real que genera tu inversiÃ³n.\n\n**InterpretaciÃ³n:**\nâ€¢ Compara la TIR con tu costo de capital\nâ€¢ TIR > Costo de capital = Buena inversiÃ³n\nâ€¢ TIR < Costo de capital = Mala inversiÃ³n"

        # Respuesta genÃ©rica
        return "**Â¡Hola! Soy Econova AI**\n\nTu asesor financiero inteligente. Actualmente estoy en modo bÃ¡sico porque los servicios de IA no estÃ¡n disponibles.\n\nPuedo ayudarte con conceptos bÃ¡sicos de finanzas. Â¿QuÃ© te gustarÃ­a saber sobre VAN, TIR o WACC?"

    def _aplicar_copiloto_adaptativo(self, respuesta: str, contexto: Dict, nivel: str, analysis_context: Dict = None) -> str:
        """
        Aplica funcionalidades del copiloto adaptativo:
        - GuÃ­a onboarding
        - Interpreta resultados
        - Sugiere acciones
        """
        if not contexto or not isinstance(contexto, dict):
            return respuesta

        sim_type = contexto.get('type')

        # Copiloto para onboarding
        if sim_type and "nuevo" in contexto.get('estado', '').lower():
            respuesta += "\n\n**ğŸš€ GuÃ­a de Inicio RÃ¡pido:**\n"
            respuesta += "â€¢ Completa tu primera simulaciÃ³n financiera\n"
            respuesta += "â€¢ Explora los diferentes tipos de anÃ¡lisis\n"
            respuesta += "â€¢ Configura tu perfil de inversionista\n"
            respuesta += "â€¢ Ãšnete a grupos de benchmarking"

        # Copiloto para interpretaciÃ³n de resultados
        elif sim_type == 'VAN' and contexto.get('van'):
            van_valor = contexto.get('van', '0')
            try:
                van_num = float(van_valor.replace('S/', '').replace(',', '').strip())
                if van_num > 0:
                    respuesta += "\n\n**ğŸ’¡ Recomendaciones para tu VAN positivo:**\n"
                    respuesta += "â€¢ Considera escalar la inversiÃ³n si es posible\n"
                    respuesta += "â€¢ EvalÃºa riesgos que puedan afectar este resultado\n"
                    respuesta += "â€¢ Compara con alternativas de inversiÃ³n"
                elif van_num < 0:
                    respuesta += "\n\n**âš ï¸ Tu VAN es negativo. Considera:**\n"
                    respuesta += "â€¢ Revisar los flujos de caja proyectados\n"
                    respuesta += "â€¢ Reducir la inversiÃ³n inicial\n"
                    respuesta += "â€¢ Buscar financiamiento mÃ¡s eficiente"
            except:
                pass

        # Copiloto para sugerencias de acciones
        respuesta += "\n\n**ğŸ¯ PrÃ³ximos pasos sugeridos:**\n"
        if nivel == "basico":
            respuesta += "â€¢ Realiza tu primera simulaciÃ³n financiera\n"
            respuesta += "â€¢ Aprende sobre VAN, TIR y WACC\n"
            respuesta += "â€¢ Explora casos de estudio sencillos"
        elif nivel == "intermedio":
            respuesta += "â€¢ Profundiza en anÃ¡lisis de sensibilidad\n"
            respuesta += "â€¢ Compara diferentes escenarios\n"
            respuesta += "â€¢ EvalÃºa riesgos de tu proyecto"
        else:  # experto
            respuesta += "â€¢ Realiza anÃ¡lisis avanzados con Monte Carlo\n"
            respuesta += "â€¢ Optimiza tu estructura de capital\n"
            respuesta += "â€¢ EvalÃºa opciones reales de inversiÃ³n"

        return respuesta

    def _aplicar_respuestas_especificas(self, respuesta: str, analysis_context: Dict, mensaje_usuario: str) -> str:
        """
        Aplica respuestas especÃ­ficas para consultas sobre valores calculados,
        reemplazando explicaciones genÃ©ricas con referencias especÃ­ficas.
        """
        if not analysis_context or not isinstance(analysis_context, dict):
            return respuesta

        tipo_analisis = analysis_context.get('tipo_analisis')
        resultados = analysis_context.get('resultados', {})

        if tipo_analisis == 'tir' and resultados.get('tir'):
            tir_valor = resultados['tir']

            # Check if user is asking about TIR interpretation AND we have TIR analysis context
            user_asking_about_tir = any(word in mensaje_usuario.lower() for word in [
                'tir', 'tasa interna', 'interpretar', 'cÃ³mo interpretar', 'quÃ© significa'
            ])

            # For TIR analysis context, ALWAYS replace generic TIR explanations with specific ones
            # Check if response contains any generic TIR explanation patterns
            generic_patterns = [
                'tasa interna de retorno',
                'tir es un concepto',
                'la tir es',
                'quÃ© es la tir',
                'tir significa',
                'tir, o tasa interna'
            ]

            is_generic_tir_response = any(pattern in respuesta.lower() for pattern in generic_patterns)

            if is_generic_tir_response or user_asking_about_tir:
                logger.info(f"ğŸ”„ Replacing generic TIR response with specific one for TIR={tir_valor}%")

                # Crear respuesta especÃ­fica basada en el valor calculado
                respuesta_especifica = f"**InterpretaciÃ³n de tu TIR del {tir_valor}%**\n\n"
                respuesta_especifica += f"Tu TIR calculada del [blue]{tir_valor}%[/blue] significa que tu inversiÃ³n genera un retorno real del {tir_valor}% anual, despuÃ©s de considerar todos los flujos de caja y el tiempo.\n\n"

                # Evaluar si es buena o no
                if tir_valor >= 20:
                    respuesta_especifica += f"Una TIR del [green]{tir_valor}%[/green] se considera [green]excelente[/green] y supera ampliamente el costo de capital tÃ­pico en PerÃº (alrededor del 12-15%).\n\n"
                elif tir_valor >= 15:
                    respuesta_especifica += f"Una TIR del [green]{tir_valor}%[/green] se considera [green]muy buena[/green] y estÃ¡ por encima del costo de capital promedio.\n\n"
                elif tir_valor >= 12:
                    respuesta_especifica += f"Una TIR del [blue]{tir_valor}%[/blue] se considera [blue]aceptable[/blue], comparable con el costo de capital en PerÃº.\n\n"
                elif tir_valor >= 8:
                    respuesta_especifica += f"Una TIR del [orange]{tir_valor}%[/orange] se considera [orange]baja[/orange] y estÃ¡ por debajo del costo de capital tÃ­pico.\n\n"
                else:
                    respuesta_especifica += f"Una TIR del [red]{tir_valor}%[/red] se considera [red]muy baja[/red] y sugiere que el proyecto podrÃ­a no ser rentable.\n\n"

                respuesta_especifica += "**Â¿QuÃ© significa esto para tu proyecto?**\n\n"
                respuesta_especifica += f"â€¢ Si tu costo de capital es menor al {tir_valor}%, la inversiÃ³n es rentable\n"
                respuesta_especifica += f"â€¢ El {tir_valor}% representa el rendimiento real de tu proyecto\n"
                respuesta_especifica += f"â€¢ Compara este {tir_valor}% con otras oportunidades de inversiÃ³n"

                # Reemplazar la respuesta genÃ©rica con la especÃ­fica
                return respuesta_especifica

        return respuesta

    def _override_respuestas_tir(self, respuesta: str, analysis_context: Dict, mensaje_usuario: str) -> str:
        """
        Override selectivo para preguntas sobre TIR - solo cuando la respuesta del AI es claramente genÃ©rica
        y necesitamos proporcionar contexto especÃ­fico del cÃ¡lculo realizado.
        """
        logger.info(f"ğŸ” SELECTIVE OVERRIDE CHECK: analysis_context exists, mensaje_usuario='{mensaje_usuario}'")

        if not analysis_context or not isinstance(analysis_context, dict):
            return respuesta

        tipo_analisis = analysis_context.get('tipo_analisis')
        resultados = analysis_context.get('resultados', {})

        if tipo_analisis == 'tir' and resultados.get('tir'):
            tir_valor = resultados['tir']

            # Solo override si la respuesta del AI es MUY corta (menos de 50 caracteres)
            # Esto indica que el AI dio una respuesta demasiado breve y necesitamos contextualizar
            is_too_short_response = len(respuesta.strip()) < 50

            # Y el usuario estÃ¡ preguntando especÃ­ficamente sobre interpretaciÃ³n del resultado
            user_asking_interpretation = any(word in mensaje_usuario.lower() for word in [
                'quÃ© significa', 'que significa', 'interpretar', 'como interpretar',
                'esta tir', 'la tir', 'mi tir', 'tir calculada', 'significa'
            ])

            if is_too_short_response and user_asking_interpretation:
                logger.info(f"ğŸ¯ SELECTIVE OVERRIDE: AI response too short ({len(respuesta)} chars), providing contextual interpretation for TIR={tir_valor}%")

                # Respuesta contextual completa en texto plano con marcadores simples
                respuesta_contextual = f"""**InterpretaciÃ³n de tu TIR del [blue]{tir_valor}%[/blue]**

BasÃ¡ndome en tu cÃ¡lculo de **[blue]TIR[/blue]** del **[blue]{tir_valor}%[/blue]**, te explico quÃ© significa este resultado.

Este **[blue]{tir_valor}%[/blue]** representa el rendimiento real anual que genera tu inversiÃ³n, considerando todos los flujos de caja que proyectaste y el tiempo en que ocurren."""

                # EvaluaciÃ³n contextual detallada
                if tir_valor >= 20:
                    respuesta_contextual += f"""

Una **[blue]TIR[/blue]** del **[green]{tir_valor}%[/green]** se considera **[green]excelente[/green]** y supera ampliamente el costo de capital promedio en PerÃº (alrededor del 12-15%). Tu proyecto tiene un rendimiento excepcional."""
                elif tir_valor >= 15:
                    respuesta_contextual += f"""

Una **[blue]TIR[/blue]** del **[green]{tir_valor}%[/green]** se considera **[green]muy buena[/green]** y estÃ¡ por encima del costo de capital promedio. Es un resultado sÃ³lido."""
                elif tir_valor >= 12:
                    respuesta_contextual += f"""

Una **[blue]TIR[/blue]** del **[blue]{tir_valor}%[/blue]** se considera **[blue]aceptable[/blue]**, comparable con el costo de capital en PerÃº. Es un resultado razonable."""
                elif tir_valor >= 8:
                    respuesta_contextual += f"""

Una **[blue]TIR[/blue]** del **[orange]{tir_valor}%[/orange]** se considera **[orange]baja[/orange]** y estÃ¡ por debajo del costo de capital tÃ­pico. Merece evaluaciÃ³n adicional."""
                else:
                    respuesta_contextual += f"""

Una **[blue]TIR[/blue]** del **[red]{tir_valor}%[/red]** se considera **[red]muy baja[/red]** y sugiere que el proyecto podrÃ­a no ser rentable con los parÃ¡metros actuales."""

                respuesta_contextual += f"""

**ğŸ’¡ Â¿QuÃ© significa esto para tu proyecto?**
â€¢ Si tu costo de capital es menor al **[blue]{tir_valor}%[/blue]**, la inversiÃ³n es **[green]rentable[/green]**
â€¢ El **[blue]{tir_valor}%[/blue]** representa el rendimiento real de tu proyecto
â€¢ Compara este **[blue]{tir_valor}%[/blue]** con otras oportunidades de inversiÃ³n

Â¿Te gustarÃ­a que te ayude a identificar oportunidades para mejorar este resultado, o tienes alguna duda especÃ­fica sobre cÃ³mo interpretar este **[blue]{tir_valor}%[/blue]** en el contexto de tu proyecto?

[Â¿CÃ³mo mejorar esta TIR?|Â¿Es rentable mi proyecto?|Â¿QuÃ© factores afectan la TIR?]"""

                return respuesta_contextual
            else:
                logger.info(f"âœ… AI response is adequate ({len(respuesta)} chars) or user not asking for interpretation - no override needed")

        return respuesta

    def _override_respuestas_van(self, respuesta: str, analysis_context: Dict, mensaje_usuario: str) -> str:
        """
        Override selectivo para preguntas sobre VAN
        """
        if not analysis_context or not isinstance(analysis_context, dict):
            return respuesta

        tipo_analisis = analysis_context.get('tipo_analisis')
        resultados = analysis_context.get('resultados', {})

        if tipo_analisis == 'van' and resultados.get('van') is not None:
            van_valor = resultados['van']

            is_too_short_response = len(respuesta.strip()) < 50
            user_asking_about_van = any(word in mensaje_usuario.lower() for word in [
                'van', 'valor actual', 'quÃ© significa', 'que significa', 'interpretar', 'como interpretar'
            ])

            if is_too_short_response and user_asking_about_van:
                logger.info(f"ğŸ¯ OVERRIDE VAN: Providing contextual interpretation for VAN={van_valor}")

                respuesta_contextual = f"""**InterpretaciÃ³n de tu VAN de S/ {van_valor:,.2f}**

BasÃ¡ndome en tu cÃ¡lculo de **[blue]VAN[/blue]** de **S/ {van_valor:,.2f}**, te explico quÃ© significa este resultado.

Este **[blue]VAN[/blue]** representa el beneficio neto actualizado de tu proyecto, considerando todos los flujos de caja descontados a valor presente."""

                if van_valor > 0:
                    respuesta_contextual += f"""

Un **[blue]VAN[/blue]** **[green]positivo[/green]** de **S/ {van_valor:,.2f}** indica que tu proyecto es **[green]rentable[/green]** y generarÃ¡ un beneficio neto superior a la inversiÃ³n inicial."""
                elif van_valor < 0:
                    respuesta_contextual += f"""

Un **[blue]VAN[/blue]** **[red]negativo[/red]** de **S/ {van_valor:,.2f}** indica que tu proyecto **[red]no es rentable[/red]** y destruirÃ¡ valor."""
                else:
                    respuesta_contextual += f"""

Un **[blue]VAN[/blue]** de **S/ {van_valor:,.2f}** indica el **[orange]punto de equilibrio[/orange]** donde el proyecto ni gana ni pierde valor."""

                respuesta_contextual += f"""

**ğŸ’¡ Â¿QuÃ© significa esto para tu proyecto?**
â€¢ **[blue]VAN > 0[/blue]**: Proyecto **[green]viable financieramente[/green]**
â€¢ **[blue]VAN < 0[/blue]**: Proyecto **[red]no viable[/red]**, requiere revisiÃ³n
â€¢ **[blue]VAN = 0[/blue]**: Punto de equilibrio, decisiÃ³n depende de otros factores

Â¿Te gustarÃ­a explorar escenarios alternativos para mejorar este VAN, o tienes alguna duda especÃ­fica sobre su interpretaciÃ³n?

[Â¿CÃ³mo mejorar el VAN?|Â¿QuÃ© factores afectan el VAN?|Â¿Es rentable mi proyecto?]"""

                return respuesta_contextual

        return respuesta

    def _override_respuestas_wacc(self, respuesta: str, analysis_context: Dict, mensaje_usuario: str) -> str:
        """
        Override selectivo para preguntas sobre WACC
        """
        if not analysis_context or not isinstance(analysis_context, dict):
            return respuesta

        tipo_analisis = analysis_context.get('tipo_analisis')
        resultados = analysis_context.get('resultados', {})

        if tipo_analisis == 'wacc' and resultados.get('wacc') is not None:
            wacc_valor = resultados['wacc']

            is_too_short_response = len(respuesta.strip()) < 50
            user_asking_about_wacc = any(word in mensaje_usuario.lower() for word in [
                'wacc', 'costo capital', 'quÃ© significa', 'que significa', 'interpretar', 'como interpretar'
            ])

            if is_too_short_response and user_asking_about_wacc:
                logger.info(f"ğŸ¯ OVERRIDE WACC: Providing contextual interpretation for WACC={wacc_valor}%")

                respuesta_contextual = f"""**InterpretaciÃ³n de tu WACC del [red]{wacc_valor}%[/red]**

Tu **[red]WACC[/red]** calculado es del **[red]{wacc_valor}%[/red]**, que representa el costo promedio ponderado de tu capital.

Este **[red]WACC[/red]** es la tasa mÃ­nima de retorno que deben generar tus proyectos para crear valor para los inversionistas."""

                if wacc_valor < 12:
                    respuesta_contextual += f"""

Un **[red]WACC[/red]** del **[green]{wacc_valor}%[/green]** se considera **[green]relativamente bajo[/green]**, lo que facilita la rentabilidad de proyectos."""
                elif wacc_valor < 15:
                    respuesta_contextual += f"""

Un **[red]WACC[/red]** del **[blue]{wacc_valor}%[/blue]** estÃ¡ en el **[blue]rango promedio[/blue]** del mercado peruano."""
                else:
                    respuesta_contextual += f"""

Un **[red]WACC[/red]** del **[orange]{wacc_valor}%[/orange]** se considera **[orange]elevado[/orange]**, lo que hace mÃ¡s difÃ­cil la rentabilidad de proyectos."""

                respuesta_contextual += f"""

**ğŸ’¡ Â¿QuÃ© significa esto para tu empresa?**
â€¢ Proyectos con **[blue]TIR > {wacc_valor}%[/blue]** son candidatos viables
â€¢ El **[red]WACC[/red]** es tu "tasa de descuento" para calcular VAN
â€¢ Un **[red]WACC[/red]** mÃ¡s bajo mejora las oportunidades de inversiÃ³n

Â¿Te gustarÃ­a explorar estrategias para reducir tu WACC o analizar cÃ³mo usarlo en evaluaciones de proyectos?

[Â¿CÃ³mo reducir el WACC?|Â¿CÃ³mo usar este WACC?|Â¿Es alto o bajo este costo de capital?]"""

                return respuesta_contextual

        return respuesta

    def _override_respuestas_portafolio(self, respuesta: str, analysis_context: Dict, mensaje_usuario: str) -> str:
        """
        Override selectivo para preguntas sobre portafolio
        """
        if not analysis_context or not isinstance(analysis_context, dict):
            return respuesta

        tipo_analisis = analysis_context.get('tipo_analisis')
        resultados = analysis_context.get('resultados', {})

        if tipo_analisis == 'portafolio' and resultados.get('retorno') is not None:
            retorno = resultados.get('retorno', 0)
            riesgo = resultados.get('riesgo', 0)
            sharpe = resultados.get('sharpe', 0)

            is_too_short_response = len(respuesta.strip()) < 50
            user_asking_about_portafolio = any(word in mensaje_usuario.lower() for word in [
                'portafolio', 'riesgo', 'retorno', 'sharpe', 'quÃ© significa', 'que significa', 'interpretar'
            ])

            if is_too_short_response and user_asking_about_portafolio:
                logger.info(f"ğŸ¯ OVERRIDE PORTAFOLIO: Providing contextual interpretation for Portfolio")

                respuesta_contextual = f"""**InterpretaciÃ³n de tu Portafolio Optimizado**

Tu portafolio tiene un retorno esperado del **[green]{retorno}%[/green]** con un riesgo del **[orange]{riesgo}%[/orange]** (desviaciÃ³n estÃ¡ndar).

El **[blue]Ratio Sharpe[/blue]** calculado es **[blue]{sharpe:.2f}[/blue]**, que mide la eficiencia riesgo-retorno de tu inversiÃ³n."""

                if sharpe > 1:
                    respuesta_contextual += f"""

Tu portafolio tiene una **[green]excelente eficiencia[/green]** con un Ratio Sharpe superior a 1."""
                elif sharpe > 0.5:
                    respuesta_contextual += f"""

Tu portafolio tiene una **[blue]buena eficiencia[/blue]** riesgo-retorno."""
                else:
                    respuesta_contextual += f"""

Tu portafolio requiere **[orange]optimizaciÃ³n[/orange]** para mejorar la relaciÃ³n riesgo-retorno."""

                respuesta_contextual += f"""

**ğŸ’¡ AnÃ¡lisis de tu portafolio:**
â€¢ **[green]Retorno esperado[/green]**: {retorno}% anual
â€¢ **[orange]Riesgo (volatilidad)[/orange]**: {riesgo}% anual  
â€¢ **[blue]Ratio Sharpe[/blue]**: {sharpe:.2f} (eficiencia)

Â¿Te gustarÃ­a explorar estrategias de diversificaciÃ³n adicionales o analizar escenarios de mercado alternativos?

[Â¿CÃ³mo diversificar mejor?|Â¿QuÃ© recomendaciones tienes?|Â¿CuÃ¡l es el riesgo Ã³ptimo?]"""

                return respuesta_contextual

        return respuesta

    def _override_respuestas_ml(self, respuesta: str, analysis_context: Dict, mensaje_usuario: str) -> str:
        """
        Override selectivo para preguntas sobre anÃ¡lisis ML
        """
        if not analysis_context or not isinstance(analysis_context, dict):
            return respuesta

        tipo_analisis = analysis_context.get('tipo_analisis')
        resultados = analysis_context.get('resultados', {})

        if tipo_analisis in ['prediccion', 'montecarlo', 'tornado', 'sensibilidad']:
            is_too_short_response = len(respuesta.strip()) < 50
            user_asking_about_ml = any(word in mensaje_usuario.lower() for word in [
                'predicciones', 'montecarlo', 'tornado', 'sensibilidad', 'anÃ¡lisis', 'resultados',
                'quÃ© significa', 'que significa', 'interpretar', 'como interpretar'
            ])

            if is_too_short_response and user_asking_about_ml:
                logger.info(f"ğŸ¯ OVERRIDE ML: Providing contextual interpretation for {tipo_analisis}")

                if tipo_analisis == 'prediccion':
                    precision = resultados.get('precision', 0)
                    tendencia = resultados.get('tendencia_principal', 'tendencias mixtas')

                    respuesta_contextual = f"""**InterpretaciÃ³n de tus Predicciones ML**

El modelo de Machine Learning generÃ³ predicciones con una **[blue]precisiÃ³n del {precision}%[/blue]**.

Las tendencias identificadas muestran **{tendencia}** para los prÃ³ximos perÃ­odos."""

                    if precision > 85:
                        respuesta_contextual += f"""

La **[green]alta precisiÃ³n[/green]** del modelo hace que estas predicciones sean **[green]muy confiables[/green]** para la toma de decisiones."""
                    elif precision > 70:
                        respuesta_contextual += f"""

La **[blue]buena precisiÃ³n[/blue]** del modelo proporciona informaciÃ³n **[blue]Ãºtil[/blue]** como referencia."""
                    else:
                        respuesta_contextual += f"""

La precisiÃ³n limitada sugiere usar estas predicciones con **[orange]precauciÃ³n[/orange]** y combinarlas con otros anÃ¡lisis."""

                    respuesta_contextual += f"""

Â¿Te gustarÃ­a explorar estrategias basadas en estas predicciones o profundizar en algÃºn aspecto especÃ­fico del anÃ¡lisis?

[Â¿CÃ³mo usar estas predicciones?|Â¿QuÃ© riesgos debo considerar?|Â¿QuÃ© estrategias recomiendas?]"""

                elif tipo_analisis == 'montecarlo':
                    van_promedio = resultados.get('van_promedio', 0)
                    probabilidad_positivo = resultados.get('probabilidad_positivo', 0)

                    respuesta_contextual = f"""**InterpretaciÃ³n de tu SimulaciÃ³n Monte Carlo**

La simulaciÃ³n generÃ³ un **[blue]VAN promedio[/blue]** de **S/ {van_promedio:,.2f}** con una **[blue]probabilidad de VAN positivo[/blue]** del **[green]{probabilidad_positivo}%[/green]**."""

                    if probabilidad_positivo > 80:
                        respuesta_contextual += f"""

La **[green]alta probabilidad de Ã©xito[/green]** indica un proyecto **[green]muy robusto[/green]** ante la incertidumbre."""
                    elif probabilidad_positivo > 60:
                        respuesta_contextual += f"""

La **[blue]probabilidad aceptable[/blue]** sugiere que el proyecto es **[blue]viable[/blue]** pero requiere gestiÃ³n de riesgos."""
                    else:
                        respuesta_contextual += f"""

La **[red]baja probabilidad[/red]** indica necesidad de **[orange]revisar los parÃ¡metros[/orange]** del proyecto."""

                    respuesta_contextual += f"""

Â¿Te gustarÃ­a analizar escenarios especÃ­ficos o explorar estrategias de mitigaciÃ³n de riesgos?

[Â¿CÃ³mo interpretar estos resultados?|Â¿QuÃ© riesgos debo considerar?|Â¿QuÃ© estrategias recomiendas?]"""

                elif tipo_analisis == 'tornado':
                    variable_mas_sensible = resultados.get('variable_mas_sensible', 'N/A')
                    impacto_maximo = resultados.get('impacto_maximo', 0)

                    respuesta_contextual = f"""**InterpretaciÃ³n de tu AnÃ¡lisis Tornado**

La variable mÃ¡s sensible identificada es **"{variable_mas_sensible}"** con un **[red]impacto mÃ¡ximo[/red]** del **[red]{impacto_maximo}%[/red]** en el VAN."""

                    if impacto_maximo > 50:
                        respuesta_contextual += f"""

Este **[red]alto impacto[/red]** indica que **"{variable_mas_sensible}"** puede hacer **[red]inviable[/red]** el proyecto si cambia desfavorablemente."""
                    elif impacto_maximo > 25:
                        respuesta_contextual += f"""

El **[orange]impacto moderado[/orange]** sugiere monitorear de cerca **"{variable_mas_sensible}"**."""
                    else:
                        respuesta_contextual += f"""

El **[green]impacto limitado[/green]** de **"{variable_mas_sensible}"** indica **[green]baja sensibilidad[/green]** del proyecto."""

                    respuesta_contextual += f"""

Â¿Te gustarÃ­a explorar estrategias de mitigaciÃ³n para las variables crÃ­ticas identificadas?

[Â¿CÃ³mo reducir la sensibilidad?|Â¿QuÃ© planes de contingencia recomiendas?|Â¿CÃ³mo monitorear estas variables?]"""

                elif tipo_analisis == 'sensibilidad':
                    punto_equilibrio = resultados.get('punto_equilibrio', 0)
                    elasticidad_critica = resultados.get('elasticidad_critica', 0)

                    respuesta_contextual = f"""**InterpretaciÃ³n de tu AnÃ¡lisis de Sensibilidad**

El **[blue]punto de equilibrio[/blue]** identificado es de **[blue]{punto_equilibrio} unidades[/blue]**, y la **[red]elasticidad crÃ­tica[/red]** es **[red]{elasticidad_critica:.2f}[/red]**."""

                    if abs(elasticidad_critica) > 2:
                        respuesta_contextual += f"""

La **[red]alta elasticidad[/red]** indica que pequeÃ±os cambios tienen **[red]gran impacto[/red]** en los resultados."""
                    elif abs(elasticidad_critica) > 1:
                        respuesta_contextual += f"""

La **[orange]elasticidad moderada[/orange]** sugiere **[orange]sensibilidad media[/orange]** a cambios."""
                    else:
                        respuesta_contextual += f"""

La **[green]baja elasticidad[/green]** indica **[green]estabilidad[/green]** ante variaciones."""

                    respuesta_contextual += f"""

Â¿Te gustarÃ­a explorar escenarios "quÃ© pasarÃ­a si" basados en este anÃ¡lisis?

[Â¿CÃ³mo optimizar el punto de equilibrio?|Â¿QuÃ© estrategias de mitigaciÃ³n?|Â¿CÃ³mo mejorar la estabilidad?]"""

                return respuesta_contextual

        return respuesta

    def _aplicar_formato_automatico(self, respuesta: str, contexto: Dict, analysis_context: Dict, nivel: str) -> str:
        """
        Aplica formato automÃ¡tico de colores y preguntas sugeridas a las respuestas de IA
        """
        try:
            # Aplicar colores automÃ¡ticos a palabras clave
            respuesta = self._aplicar_colores_automaticos(respuesta, contexto, analysis_context)

            # Agregar preguntas sugeridas automÃ¡ticas
            respuesta = self._agregar_preguntas_sugeridas(respuesta, contexto, analysis_context, nivel)

            return respuesta
        except Exception as e:
            logger.error(f"Error aplicando formato automÃ¡tico: {e}")
            return respuesta

    def _aplicar_colores_automaticos(self, respuesta: str, contexto: Dict, analysis_context: Dict) -> str:
        """
        Aplica colores automÃ¡ticos a palabras clave importantes en la respuesta
        """
        import re

        # Colores para resultados positivos
        respuesta = re.sub(r'\bpositivo\b', '<strong style="color: #059669;">positivo</strong>', respuesta, flags=re.IGNORECASE)
        respuesta = re.sub(r'\brentable\b', '<strong style="color: #059669;">rentable</strong>', respuesta, flags=re.IGNORECASE)
        respuesta = re.sub(r'\bbeneficio\b', '<strong style="color: #059669;">beneficio</strong>', respuesta, flags=re.IGNORECASE)
        respuesta = re.sub(r'\bganancia\b', '<strong style="color: #059669;">ganancia</strong>', respuesta, flags=re.IGNORECASE)

        # Colores para resultados negativos
        respuesta = re.sub(r'\bnegativo\b', '<strong style="color: #dc2626;">negativo</strong>', respuesta, flags=re.IGNORECASE)
        respuesta = re.sub(r'\bpÃ©rdida\b', '<strong style="color: #dc2626;">pÃ©rdida</strong>', respuesta, flags=re.IGNORECASE)
        respuesta = re.sub(r'\briesgo\b', '<strong style="color: #dc2626;">riesgo</strong>', respuesta, flags=re.IGNORECASE)
        respuesta = re.sub(r'\bpeligro\b', '<strong style="color: #dc2626;">peligro</strong>', respuesta, flags=re.IGNORECASE)

        # Colores para conceptos importantes
        respuesta = re.sub(r'\bVAN\b', '<strong style="color: #2563eb;">VAN</strong>', respuesta)
        respuesta = re.sub(r'\bTIR\b', '<strong style="color: #7c3aed;">TIR</strong>', respuesta)
        respuesta = re.sub(r'\bWACC\b', '<strong style="color: #dc2626;">WACC</strong>', respuesta)

        # Colores para advertencias
        respuesta = re.sub(r'\badvertencia\b', '<strong style="color: #ea580c;">advertencia</strong>', respuesta, flags=re.IGNORECASE)
        respuesta = re.sub(r'\bconsidera\b', '<strong style="color: #ea580c;">considera</strong>', respuesta, flags=re.IGNORECASE)
        respuesta = re.sub(r'\brecomienda\b', '<strong style="color: #ea580c;">recomienda</strong>', respuesta, flags=re.IGNORECASE)

        # Colores para valores monetarios
        respuesta = re.sub(r'(S/\s*\d{1,3}(?:\.\d{3})*(?:,\d{2})?)', r'<strong style="color: #059669;">\1</strong>', respuesta)

        return respuesta

    def _agregar_preguntas_sugeridas(self, respuesta: str, contexto: Dict, analysis_context: Dict, nivel: str) -> str:
        """
        Agrega preguntas sugeridas automÃ¡ticas basadas en el contexto
        """
        preguntas_sugeridas = []

        # Determinar tipo de anÃ¡lisis o contexto
        tipo_analisis = None
        if analysis_context and isinstance(analysis_context, dict):
            tipo_analisis = analysis_context.get('tipo_analisis')
        elif contexto and isinstance(contexto, dict):
            tipo_analisis = contexto.get('type')

        # Preguntas basadas en el tipo de anÃ¡lisis
        if tipo_analisis == 'van':
            preguntas_sugeridas = [
                "Â¿CÃ³mo mejorar el VAN de mi proyecto?",
                "Â¿QuÃ© factores afectan mÃ¡s el VAN?",
                "Â¿Es rentable mi inversiÃ³n?"
            ]
        elif tipo_analisis == 'tir':
            preguntas_sugeridas = [
                "Â¿QuÃ© significa esta TIR?",
                "Â¿CÃ³mo comparar con otras inversiones?",
                "Â¿Es buena esta tasa de retorno?"
            ]
        elif tipo_analisis == 'wacc':
            preguntas_sugeridas = [
                "Â¿CÃ³mo usar este WACC en mis cÃ¡lculos?",
                "Â¿Es alto o bajo este costo de capital?",
                "Â¿CÃ³mo reducir mi WACC?"
            ]
        elif tipo_analisis == 'portafolio':
            preguntas_sugeridas = [
                "Â¿CÃ³mo diversificar mejor mi portafolio?",
                "Â¿CuÃ¡l es el riesgo de mis inversiones?",
                "Â¿QuÃ© recomendaciones tienes para optimizar?"
            ]
        elif tipo_analisis in ['prediccion', 'montecarlo', 'tornado', 'escenarios']:
            preguntas_sugeridas = [
                "Â¿CÃ³mo interpretar estos resultados?",
                "Â¿QuÃ© riesgos debo considerar?",
                "Â¿QuÃ© estrategias recomiendas?"
            ]
        else:
            # Solo usar detecciÃ³n basada en contenido si no hay tipo_analisis especÃ­fico
            # Priorizar el tipo de anÃ¡lisis sobre el contenido de la respuesta
            if 'van' in respuesta.lower() and not tipo_analisis:
                preguntas_sugeridas = [
                    "Â¿QuÃ© es el VAN?",
                    "Â¿CÃ³mo calcular el VAN?",
                    "Â¿CÃ³mo interpretar el VAN?"
                ]
            elif 'tir' in respuesta.lower() and not tipo_analisis:
                preguntas_sugeridas = [
                    "Â¿QuÃ© es la TIR?",
                    "Â¿CÃ³mo calcular la TIR?",
                    "Â¿CÃ³mo usar la TIR?"
                ]
            elif 'wacc' in respuesta.lower() and not tipo_analisis:
                preguntas_sugeridas = [
                    "Â¿QuÃ© es el WACC?",
                    "Â¿CÃ³mo calcular el WACC?",
                    "Â¿Para quÃ© usar el WACC?"
                ]
            else:
                preguntas_sugeridas = [
                    "Â¿Puedes explicarme mejor?",
                    "Â¿Tienes un ejemplo prÃ¡ctico?",
                    "Â¿CuÃ¡les son las limitaciones?"
                ]

        # Agregar preguntas sugeridas al final de la respuesta
        if preguntas_sugeridas:
            respuesta += f"\n\n[{'|'.join(preguntas_sugeridas)}]"

        return respuesta

    def _log_conversacion(self, usuario_id: Optional[int], mensaje_usuario: str,
                         respuesta_ia: str, proveedor: str, contexto: Dict, nivel: str, analysis_context: Dict = None):
        """
        Registra la conversaciÃ³n en la base de datos
        """
        try:
            db = get_db_connection()
            cursor = db.cur

            # Crear tabla si no existe
            cursor.execute("""
                CREATE TABLE IF NOT EXISTS Conversaciones_Chatbot (
                    conversacion_id INTEGER PRIMARY KEY AUTOINCREMENT,
                    usuario_id INTEGER,
                    mensaje_usuario TEXT NOT NULL,
                    respuesta_ia TEXT NOT NULL,
                    proveedor_ia VARCHAR(20) DEFAULT 'groq',
                    nivel_usuario VARCHAR(20) DEFAULT 'basico',
                    contexto TEXT,
                    tipo_interaccion VARCHAR(50),
                    fecha TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                    FOREIGN KEY (usuario_id) REFERENCES Usuarios(usuario_id)
                )
            """)

            # Determinar tipo de interacciÃ³n
            tipo_interaccion = "consulta_general"
            if contexto and isinstance(contexto, dict):
                sim_type = contexto.get('type')
                if sim_type:
                    tipo_interaccion = f"simulacion_{sim_type.lower()}"
                elif any(word in mensaje_usuario.lower() for word in ["van", "tir", "wacc", "valor", "tasa", "retorno"]):
                    tipo_interaccion = "consulta_tecnica"

            # Insertar conversaciÃ³n
            cursor.execute("""
                INSERT INTO Conversaciones_Chatbot
                (usuario_id, mensaje_usuario, respuesta_ia, proveedor_ia, nivel_usuario, contexto, tipo_interaccion)
                VALUES (?, ?, ?, ?, ?, ?, ?)
            """, (
                usuario_id,
                mensaje_usuario,
                respuesta_ia,
                proveedor,
                nivel,
                json.dumps(contexto) if contexto else None,
                tipo_interaccion
            ))

            db.commit()

            logger.info(f"âœ… ConversaciÃ³n logged - Usuario: {usuario_id}, Proveedor: {proveedor}, Nivel: {nivel}")

        except Exception as e:
            logger.error(f"âŒ Error logging conversaciÃ³n: {e}")

    def obtener_estadisticas_conversaciones(self, usuario_id: Optional[int] = None) -> Dict[str, Any]:
        """
        Obtiene estadÃ­sticas de conversaciones para anÃ¡lisis
        """
        try:
            db = get_db_connection()
            cursor = db.cur

            if usuario_id:
                # EstadÃ­sticas por usuario
                cursor.execute("""
                    SELECT
                        COUNT(*) as total_conversaciones,
                        COUNT(DISTINCT DATE(fecha)) as dias_activos,
                        AVG(LENGTH(mensaje_usuario)) as longitud_promedio_mensajes,
                        proveedor_ia,
                        COUNT(*) as uso_por_proveedor
                    FROM Conversaciones_Chatbot
                    WHERE usuario_id = ?
                    GROUP BY proveedor_ia
                """, (usuario_id,))

                stats_proveedores = cursor.fetchall()

                cursor.execute("""
                    SELECT tipo_interaccion, COUNT(*) as cantidad
                    FROM Conversaciones_Chatbot
                    WHERE usuario_id = ?
                    GROUP BY tipo_interaccion
                    ORDER BY cantidad DESC
                """, (usuario_id,))

                tipos_interaccion = cursor.fetchall()

            else:
                # EstadÃ­sticas globales
                cursor.execute("""
                    SELECT COUNT(*) as total_conversaciones,
                           COUNT(DISTINCT usuario_id) as usuarios_unicos,
                           AVG(LENGTH(mensaje_usuario)) as longitud_promedio
                    FROM Conversaciones_Chatbot
                """)

                stats_globales = cursor.fetchone()
                stats_proveedores = []
                tipos_interaccion = []

            return {
                "estadisticas_proveedores": stats_proveedores,
                "tipos_interaccion": tipos_interaccion,
                "fecha_generacion": datetime.now().isoformat()
            }

        except Exception as e:
            logger.error(f"Error obteniendo estadÃ­sticas: {e}")
            return {"error": str(e)}

# Instancia global del servicio
chatbot_servicio = ChatbotServicio()

def obtener_servicio_chatbot() -> ChatbotServicio:
    """Factory function para obtener instancia del servicio"""
    return chatbot_servicio
