"""
Servicio de Chatbot Inteligente - Versi√≥n Mejorada
Implementa Groq + OpenAI fallback, prompts multinivel, logs conversacionales, 
copiloto adaptativo, cach√© inteligente, memoria conversacional y validaci√≥n avanzada
"""

import os
import json
import logging
import hashlib
import html
import re
from datetime import datetime, timedelta
from typing import Dict, List, Optional, Any
from functools import lru_cache
import traceback

# AI Providers
try:
    from groq import Groq
    GROQ_AVAILABLE = True
except ImportError:
    GROQ_AVAILABLE = False
    Groq = None

try:
    import openai
    OPENAI_AVAILABLE = True
except ImportError:
    OPENAI_AVAILABLE = False
    openai = None

from ..utils.base_datos import get_db_connection
from .chatbot_prompts import ChatbotPrompts

# Configure logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class ChatbotServicio:
    """
    Servicio principal del chatbot con funcionalidades avanzadas:
    - Groq + OpenAI fallback con retry autom√°tico
    - Prompts multinivel (b√°sico/intermedio/expert)
    - Logs conversacionales
    - Copiloto adaptativo
    - Sistema de cach√© inteligente
    - Memoria conversacional mejorada
    - Validaci√≥n y sanitizaci√≥n de mensajes
    - Analytics y m√©tricas
    """

    def __init__(self):
        self.groq_client = None
        self.openai_client = None
        self.response_cache = {}  # Cache de respuestas con TTL
        self.cache_ttl = timedelta(hours=1)  # Tiempo de vida del cach√©
        self._initialize_clients()

    def _initialize_clients(self):
        """Inicializar clientes de IA con fallback"""
        # Initialize Groq
        if GROQ_AVAILABLE:
            groq_key = os.getenv("GROQ_API_KEY", "")
            if groq_key:
                try:
                    self.groq_client = Groq(api_key=groq_key)
                    logger.info("‚úÖ Groq client inicializado")
                except Exception as e:
                    logger.error(f"‚ùå Error inicializando Groq: {e}")
                    self.groq_client = None

        # Initialize OpenAI
        if OPENAI_AVAILABLE:
            openai_key = os.getenv("OPENAI_API_KEY", "")
            if openai_key:
                try:
                    self.openai_client = openai.OpenAI(api_key=openai_key)
                    logger.info("‚úÖ OpenAI client inicializado")
                except Exception as e:
                    logger.error(f"‚ùå Error inicializando OpenAI: {e}")
                    self.openai_client = None

        if not self.groq_client and not self.openai_client:
            logger.warning("‚ö†Ô∏è Ning√∫n cliente de IA disponible - usando modo fallback")
    
    def validar_mensaje(self, mensaje: str) -> Dict[str, Any]:
        """
        Valida y sanitiza el mensaje del usuario
        """
        if not mensaje or len(mensaje.strip()) == 0:
            return {"valid": False, "error": "El mensaje no puede estar vac√≠o"}
        
        if len(mensaje) > 2000:
            return {"valid": False, "error": "El mensaje es demasiado largo (m√°ximo 2000 caracteres)"}
        
        # Detectar contenido inapropiado b√°sico
        palabras_prohibidas = ['spam', 'hack', 'exploit', 'virus', 'malware']
        mensaje_lower = mensaje.lower()
        if any(palabra in mensaje_lower for palabra in palabras_prohibidas):
            return {"valid": False, "error": "El mensaje contiene contenido no permitido"}
        
        # Sanitizar HTML
        mensaje_sanitizado = html.escape(mensaje)
        
        return {
            "valid": True,
            "sanitized": mensaje_sanitizado,
            "original_length": len(mensaje)
        }
    
    def _get_cache_key(self, mensaje: str, nivel: str, analysis_context: Dict = None) -> str:
        """Genera clave √∫nica para cach√©"""
        context_str = json.dumps(analysis_context, sort_keys=True) if analysis_context else ""
        key_string = f"{mensaje}_{nivel}_{context_str}"
        return hashlib.md5(key_string.encode()).hexdigest()
    
    def _get_cached_response(self, cache_key: str) -> Optional[Dict[str, Any]]:
        """Obtiene respuesta del cach√© si existe y no ha expirado"""
        if cache_key in self.response_cache:
            cached_data = self.response_cache[cache_key]
            cache_time = datetime.fromisoformat(cached_data.get('cache_timestamp', ''))
            
            if datetime.now() - cache_time < self.cache_ttl:
                logger.info("‚úÖ Respuesta obtenida de cach√©")
                return cached_data
            else:
                # Eliminar cach√© expirado
                del self.response_cache[cache_key]
        
        return None
    
    def _save_to_cache(self, cache_key: str, response_data: Dict[str, Any]):
        """Guarda respuesta en cach√©"""
        response_data['cache_timestamp'] = datetime.now().isoformat()
        self.response_cache[cache_key] = response_data
        
        # Limpiar cach√© antiguo peri√≥dicamente (mantener solo √∫ltimos 100)
        if len(self.response_cache) > 100:
            # Eliminar entradas m√°s antiguas
            sorted_cache = sorted(
                self.response_cache.items(),
                key=lambda x: x[1].get('cache_timestamp', ''),
                reverse=True
            )
            self.response_cache = dict(sorted_cache[:100])
    
    def obtener_historial_conversacion(self, usuario_id: int, limit: int = 5) -> List[Dict]:
        """
        Obtiene historial reciente de conversaci√≥n para contexto
        """
        if not usuario_id:
            return []
        
        try:
            db = get_db_connection()
            cursor = db.cur
            
            cursor.execute("""
                SELECT mensaje_usuario, respuesta_ia, fecha
                FROM Conversaciones_Chatbot
                WHERE usuario_id = ?
                ORDER BY fecha DESC
                LIMIT ?
            """, (usuario_id, limit))
            
            historial = []
            for row in cursor.fetchall():
                historial.append({
                    'usuario': row[0],
                    'bot': row[1][:300] if row[1] else '',  # Limitar longitud
                    'fecha': row[2]
                })
            
            return list(reversed(historial))  # Orden cronol√≥gico
        except Exception as e:
            logger.error(f"Error obteniendo historial: {e}")
            return []

    def determinar_nivel_usuario(self, usuario_id: Optional[int] = None,
                               historial_conversaciones: List[Dict] = None) -> str:
        """
        Determina el nivel de expertise del usuario basado en su historial
        """
        if not usuario_id:
            return "basico"

        try:
            conn = get_db_connection().conn
            cursor = conn.cursor()

            # Analizar historial de conversaciones
            cursor.execute("""
                SELECT COUNT(*) as total_conversaciones,
                       AVG(LENGTH(mensaje_usuario)) as longitud_promedio,
                       COUNT(CASE WHEN tipo_interaccion = 'simulacion_financiera' THEN 1 END) as simulaciones,
                       COUNT(CASE WHEN tipo_interaccion = 'consulta_tecnica' THEN 1 END) as consultas_tecnicas
                FROM Conversaciones_Chatbot
                WHERE usuario_id = ? AND fecha > datetime('now', '-30 days')
            """, (usuario_id,))

            stats = cursor.fetchone()

            if stats:
                total_conv, long_promedio, simulaciones, consultas_tecnicas = stats

                # L√≥gica de clasificaci√≥n
                if total_conv >= 20 and (simulaciones >= 5 or consultas_tecnicas >= 10):
                    return "experto"
                elif total_conv >= 10 and (simulaciones >= 2 or consultas_tecnicas >= 3):
                    return "intermedio"
                else:
                    return "basico"

            conn.close()

        except Exception as e:
            logger.error(f"Error determinando nivel usuario: {e}")

        return "basico"

    def obtener_prompt_por_nivel(self, nivel: str, contexto: Dict = None, analysis_context: Dict = None) -> str:
        """
        Retorna el prompt apropiado seg√∫n el nivel del usuario usando el sistema de prompts mejorado
        """
        # Obtener prompt base del sistema mejorado
        prompt = ChatbotPrompts.BASE_PROMPTS.get(nivel, ChatbotPrompts.BASE_PROMPTS["basico"])

        # Agregar contexto de an√°lisis espec√≠fico usando el sistema mejorado
        if analysis_context and isinstance(analysis_context, dict):
            tipo_analisis = analysis_context.get('tipo_analisis')
            resultados = analysis_context.get('resultados', {})
            
            # Usar el sistema de prompts contextual mejorado
            if tipo_analisis and resultados:
                contextual_prompt = ChatbotPrompts.get_contextual_prompt(tipo_analisis, resultados)
                if contextual_prompt:
                    prompt += "\n\n" + contextual_prompt
                
                # Agregar informaci√≥n adicional de resultados para contexto
                prompt += f"\n\nRESULTADOS ESPEC√çFICOS DEL AN√ÅLISIS:"
                for key, value in resultados.items():
                    if value is not None and key not in ['escenarios_originales']:
                        if isinstance(value, (int, float)):
                            if 'porcentaje' in key.lower() or 'tir' in key.lower() or 'wacc' in key.lower() or 'rendimiento' in key.lower() or 'riesgo' in key.lower():
                                prompt += f"\n- {key}: {value}%"
                            elif 'probabilidad' in key.lower():
                                prompt += f"\n- {key}: {(value * 100):.1f}%"
                            else:
                                prompt += f"\n- {key}: S/ {value:,.2f}" if value > 1000 else f"\n- {key}: {value}"
                        elif isinstance(value, str):
                            prompt += f"\n- {key}: {value}"

        # Agregar contexto espec√≠fico legacy si est√° disponible
        elif contexto and isinstance(contexto, dict):
            sim_type = contexto.get('type')
            if sim_type:
                prompt += f"\n\nCONTEXTO ACTUAL: El usuario est√° trabajando en un an√°lisis de {sim_type}."
                if contexto.get('van'):
                    prompt += f" VAN calculado: {contexto['van']}"
                if contexto.get('tir'):
                    prompt += f" TIR calculada: {contexto['tir']}"

        return prompt

    def consultar_ia(self, mensaje: str, usuario_id: Optional[int] = None,
                    contexto: Dict = None, analysis_context: Dict = None) -> Dict[str, Any]:
        """
        M√©todo principal para consultar IA con fallback y funcionalidades avanzadas
        """
        try:
            start_time = datetime.now()
            
            # Validar mensaje
            validacion = self.validar_mensaje(mensaje)
            if not validacion["valid"]:
                return {
                    "success": False,
                    "error": validacion["error"],
                    "respuesta": f"Lo siento, {validacion['error'].lower()}"
                }
            
            mensaje = validacion["sanitized"]
            
            # Debug: Log analysis_context
            logger.info(f"üîç Analysis context received: {analysis_context}")

            # Determinar nivel del usuario
            nivel_usuario = self.determinar_nivel_usuario(usuario_id)

            # Verificar cach√©
            cache_key = self._get_cache_key(mensaje, nivel_usuario, analysis_context)
            cached_response = self._get_cached_response(cache_key)
            if cached_response:
                return cached_response

            # Obtener historial de conversaci√≥n para contexto
            historial_contexto = ""
            if usuario_id:
                historial = self.obtener_historial_conversacion(usuario_id, limit=3)
                if historial:
                    historial_contexto = "\n\nCONTEXTO DE CONVERSACI√ìN RECIENTE:\n"
                    for i, conv in enumerate(historial, 1):
                        historial_contexto += f"{i}. Usuario: {conv['usuario'][:150]}...\n"
                        historial_contexto += f"   Bot: {conv['bot'][:150]}...\n\n"

            # Obtener prompt apropiado con contexto de an√°lisis
            system_prompt = self.obtener_prompt_por_nivel(nivel_usuario, contexto, analysis_context)
            
            # Agregar historial al prompt
            if historial_contexto:
                system_prompt += historial_contexto

            # Intentar primero con Groq
            respuesta = None
            proveedor_usado = None

            if self.groq_client:
                try:
                    respuesta = self._consultar_groq(mensaje, system_prompt)
                    proveedor_usado = "groq"
                    logger.info("‚úÖ Respuesta obtenida de Groq")
                except Exception as e:
                    logger.warning(f"‚ùå Error con Groq, intentando OpenAI: {e}")

            # Fallback a OpenAI si Groq falla
            if not respuesta and self.openai_client:
                try:
                    respuesta = self._consultar_openai(mensaje, system_prompt)
                    proveedor_usado = "openai"
                    logger.info("‚úÖ Respuesta obtenida de OpenAI (fallback)")
                except Exception as e:
                    logger.error(f"‚ùå Error con OpenAI: {e}")

            # √öltimo fallback a respuestas predefinidas
            if not respuesta:
                respuesta = self._respuesta_fallback(mensaje, contexto, analysis_context)
                proveedor_usado = "fallback"
                logger.info("‚ö†Ô∏è Usando respuesta fallback")

            # Agregar funcionalidades del copiloto adaptativo
            respuesta = self._aplicar_copiloto_adaptativo(respuesta, contexto, nivel_usuario, analysis_context)

            # Aplicar respuestas espec√≠ficas para consultas sobre valores calculados
            respuesta = self._aplicar_respuestas_especificas(respuesta, analysis_context, mensaje)

            # Override completo para preguntas sobre TIR cuando hay contexto de an√°lisis
            respuesta = self._override_respuestas_tir(respuesta, analysis_context, mensaje)

            # Overrides para otros tipos de an√°lisis
            respuesta = self._override_respuestas_van(respuesta, analysis_context, mensaje)
            respuesta = self._override_respuestas_wacc(respuesta, analysis_context, mensaje)
            respuesta = self._override_respuestas_portafolio(respuesta, analysis_context, mensaje)
            respuesta = self._override_respuestas_ml(respuesta, analysis_context, mensaje)

            # Aplicar formato autom√°tico de colores y preguntas sugeridas
            respuesta = self._aplicar_formato_automatico(respuesta, contexto, analysis_context, nivel_usuario)

            # Calcular tiempo de respuesta
            response_time = (datetime.now() - start_time).total_seconds()
            
            # Preparar respuesta
            result = {
                "success": True,
                "respuesta": respuesta,
                "proveedor": proveedor_usado,
                "nivel_usuario": nivel_usuario,
                "timestamp": datetime.now().isoformat(),
                "response_time": response_time,
                "cached": False
            }
            
            # Guardar en cach√©
            self._save_to_cache(cache_key, result.copy())

            # Log de la conversaci√≥n (as√≠ncrono para no bloquear)
            try:
                self._log_conversacion(usuario_id, mensaje, respuesta, proveedor_usado, contexto, nivel_usuario, analysis_context)
            except Exception as e:
                logger.error(f"Error logging conversaci√≥n (no cr√≠tico): {e}")

            return result

        except Exception as e:
            logger.error(f"‚ùå Error en consultar_ia: {e}")
            return {
                "success": False,
                "error": str(e),
                "respuesta": "Lo siento, hubo un error al procesar tu consulta. Por favor, int√©ntalo de nuevo."
            }

    def _consultar_groq(self, mensaje: str, system_prompt: str, max_retries: int = 3) -> str:
        """Consulta a Groq API con retry autom√°tico"""
        for attempt in range(max_retries):
            try:
                response = self.groq_client.chat.completions.create(
                    model="llama-3.1-8b-instant",
                    messages=[
                        {"role": "system", "content": system_prompt},
                        {"role": "user", "content": mensaje}
                    ],
                    max_tokens=2000,
                    temperature=0.7,
                )
                return response.choices[0].message.content.strip()
            except Exception as e:
                if attempt < max_retries - 1:
                    logger.warning(f"Intento {attempt + 1} fallido con Groq, reintentando...: {e}")
                    import time
                    time.sleep(1 * (attempt + 1))  # Backoff exponencial
                else:
                    raise e

    def _consultar_openai(self, mensaje: str, system_prompt: str, max_retries: int = 3) -> str:
        """Consulta a OpenAI API con retry autom√°tico"""
        for attempt in range(max_retries):
            try:
                response = self.openai_client.chat.completions.create(
                    model="gpt-3.5-turbo",
                    messages=[
                        {"role": "system", "content": system_prompt},
                        {"role": "user", "content": mensaje}
                    ],
                    max_tokens=2000,
                    temperature=0.7,
                )
                return response.choices[0].message.content.strip()
            except Exception as e:
                if attempt < max_retries - 1:
                    logger.warning(f"Intento {attempt + 1} fallido con OpenAI, reintentando...: {e}")
                    import time
                    time.sleep(1 * (attempt + 1))  # Backoff exponencial
                else:
                    raise e

    def _respuesta_fallback(self, mensaje: str, contexto: Dict = None, analysis_context: Dict = None) -> str:
        """Respuestas predefinidas cuando IA no est√° disponible"""
        mensaje_lower = mensaje.lower()

        # Respuestas b√°sicas
        if "van" in mensaje_lower and ("que es" in mensaje_lower or "qu√© es" in mensaje_lower):
            return "**¬øQu√© es el VAN?**\n\nEl VAN (Valor Actual Neto) mide la rentabilidad real de una inversi√≥n, considerando el tiempo y el riesgo del dinero.\n\n**F√≥rmula b√°sica:** VAN = Flujos de caja descontados - Inversi√≥n inicial\n\n**Interpretaci√≥n:**\n‚Ä¢ VAN > 0: Inversi√≥n rentable\n‚Ä¢ VAN < 0: Inversi√≥n no rentable\n‚Ä¢ VAN = 0: Punto de equilibrio"

        if "tir" in mensaje_lower and ("que es" in mensaje_lower or "qu√© es" in mensaje_lower):
            return "**¬øQu√© es la TIR?**\n\nLa TIR (Tasa Interna de Retorno) es el porcentaje de ganancia real que genera tu inversi√≥n.\n\n**Interpretaci√≥n:**\n‚Ä¢ Compara la TIR con tu costo de capital\n‚Ä¢ TIR > Costo de capital = Buena inversi√≥n\n‚Ä¢ TIR < Costo de capital = Mala inversi√≥n"

        # Respuesta gen√©rica
        return "**¬°Hola! Soy Econova AI**\n\nTu asesor financiero inteligente. Actualmente estoy en modo b√°sico porque los servicios de IA no est√°n disponibles.\n\nPuedo ayudarte con conceptos b√°sicos de finanzas. ¬øQu√© te gustar√≠a saber sobre VAN, TIR o WACC?"

    def _aplicar_copiloto_adaptativo(self, respuesta: str, contexto: Dict, nivel: str, analysis_context: Dict = None) -> str:
        """
        Aplica funcionalidades del copiloto adaptativo:
        - Gu√≠a onboarding
        - Interpreta resultados
        - Sugiere acciones
        """
        if not contexto or not isinstance(contexto, dict):
            return respuesta

        sim_type = contexto.get('type')

        # Copiloto para onboarding
        if sim_type and "nuevo" in contexto.get('estado', '').lower():
            respuesta += "\n\n**üöÄ Gu√≠a de Inicio R√°pido:**\n"
            respuesta += "‚Ä¢ Completa tu primera simulaci√≥n financiera\n"
            respuesta += "‚Ä¢ Explora los diferentes tipos de an√°lisis\n"
            respuesta += "‚Ä¢ Configura tu perfil de inversionista\n"
            respuesta += "‚Ä¢ √önete a grupos de benchmarking"

        # Copiloto para interpretaci√≥n de resultados
        elif sim_type == 'VAN' and contexto.get('van'):
            van_valor = contexto.get('van', '0')
            try:
                van_num = float(van_valor.replace('S/', '').replace(',', '').strip())
                if van_num > 0:
                    respuesta += "\n\n**üí° Recomendaciones para tu VAN positivo:**\n"
                    respuesta += "‚Ä¢ Considera escalar la inversi√≥n si es posible\n"
                    respuesta += "‚Ä¢ Eval√∫a riesgos que puedan afectar este resultado\n"
                    respuesta += "‚Ä¢ Compara con alternativas de inversi√≥n"
                elif van_num < 0:
                    respuesta += "\n\n**‚ö†Ô∏è Tu VAN es negativo. Considera:**\n"
                    respuesta += "‚Ä¢ Revisar los flujos de caja proyectados\n"
                    respuesta += "‚Ä¢ Reducir la inversi√≥n inicial\n"
                    respuesta += "‚Ä¢ Buscar financiamiento m√°s eficiente"
            except:
                pass

        # Copiloto para sugerencias de acciones
        respuesta += "\n\n**üéØ Pr√≥ximos pasos sugeridos:**\n"
        if nivel == "basico":
            respuesta += "‚Ä¢ Realiza tu primera simulaci√≥n financiera\n"
            respuesta += "‚Ä¢ Aprende sobre VAN, TIR y WACC\n"
            respuesta += "‚Ä¢ Explora casos de estudio sencillos"
        elif nivel == "intermedio":
            respuesta += "‚Ä¢ Profundiza en an√°lisis de sensibilidad\n"
            respuesta += "‚Ä¢ Compara diferentes escenarios\n"
            respuesta += "‚Ä¢ Eval√∫a riesgos de tu proyecto"
        else:  # experto
            respuesta += "‚Ä¢ Realiza an√°lisis avanzados con Monte Carlo\n"
            respuesta += "‚Ä¢ Optimiza tu estructura de capital\n"
            respuesta += "‚Ä¢ Eval√∫a opciones reales de inversi√≥n"

        return respuesta

    def _aplicar_respuestas_especificas(self, respuesta: str, analysis_context: Dict, mensaje_usuario: str) -> str:
        """
        Aplica respuestas espec√≠ficas para consultas sobre valores calculados,
        reemplazando explicaciones gen√©ricas con referencias espec√≠ficas.
        """
        if not analysis_context or not isinstance(analysis_context, dict):
            return respuesta

        tipo_analisis = analysis_context.get('tipo_analisis')
        resultados = analysis_context.get('resultados', {})

        if tipo_analisis == 'tir' and resultados.get('tir'):
            tir_valor = resultados['tir']

            # Check if user is asking about TIR interpretation AND we have TIR analysis context
            user_asking_about_tir = any(word in mensaje_usuario.lower() for word in [
                'tir', 'tasa interna', 'interpretar', 'c√≥mo interpretar', 'qu√© significa'
            ])

            # For TIR analysis context, ALWAYS replace generic TIR explanations with specific ones
            # Check if response contains any generic TIR explanation patterns
            generic_patterns = [
                'tasa interna de retorno',
                'tir es un concepto',
                'la tir es',
                'qu√© es la tir',
                'tir significa',
                'tir, o tasa interna'
            ]

            is_generic_tir_response = any(pattern in respuesta.lower() for pattern in generic_patterns)

            if is_generic_tir_response or user_asking_about_tir:
                logger.info(f"üîÑ Replacing generic TIR response with specific one for TIR={tir_valor}%")

                # Crear respuesta espec√≠fica basada en el valor calculado
                respuesta_especifica = f"**Interpretaci√≥n de tu TIR del {tir_valor}%**\n\n"
                respuesta_especifica += f"Tu TIR calculada del [blue]{tir_valor}%[/blue] significa que tu inversi√≥n genera un retorno real del {tir_valor}% anual, despu√©s de considerar todos los flujos de caja y el tiempo.\n\n"

                # Evaluar si es buena o no
                if tir_valor >= 20:
                    respuesta_especifica += f"Una TIR del [green]{tir_valor}%[/green] se considera [green]excelente[/green] y supera ampliamente el costo de capital t√≠pico en Per√∫ (alrededor del 12-15%).\n\n"
                elif tir_valor >= 15:
                    respuesta_especifica += f"Una TIR del [green]{tir_valor}%[/green] se considera [green]muy buena[/green] y est√° por encima del costo de capital promedio.\n\n"
                elif tir_valor >= 12:
                    respuesta_especifica += f"Una TIR del [blue]{tir_valor}%[/blue] se considera [blue]aceptable[/blue], comparable con el costo de capital en Per√∫.\n\n"
                elif tir_valor >= 8:
                    respuesta_especifica += f"Una TIR del [orange]{tir_valor}%[/orange] se considera [orange]baja[/orange] y est√° por debajo del costo de capital t√≠pico.\n\n"
                else:
                    respuesta_especifica += f"Una TIR del [red]{tir_valor}%[/red] se considera [red]muy baja[/red] y sugiere que el proyecto podr√≠a no ser rentable.\n\n"

                respuesta_especifica += "**¬øQu√© significa esto para tu proyecto?**\n\n"
                respuesta_especifica += f"‚Ä¢ Si tu costo de capital es menor al {tir_valor}%, la inversi√≥n es rentable\n"
                respuesta_especifica += f"‚Ä¢ El {tir_valor}% representa el rendimiento real de tu proyecto\n"
                respuesta_especifica += f"‚Ä¢ Compara este {tir_valor}% con otras oportunidades de inversi√≥n"

                # Reemplazar la respuesta gen√©rica con la espec√≠fica
                return respuesta_especifica

        return respuesta

    def _override_respuestas_tir(self, respuesta: str, analysis_context: Dict, mensaje_usuario: str) -> str:
        """
        Override selectivo para preguntas sobre TIR - solo cuando la respuesta del AI es claramente gen√©rica
        y necesitamos proporcionar contexto espec√≠fico del c√°lculo realizado.
        """
        logger.info(f"üîç SELECTIVE OVERRIDE CHECK: analysis_context exists, mensaje_usuario='{mensaje_usuario}'")

        if not analysis_context or not isinstance(analysis_context, dict):
            return respuesta

        tipo_analisis = analysis_context.get('tipo_analisis')
        resultados = analysis_context.get('resultados', {})

        if tipo_analisis == 'tir' and resultados.get('tir'):
            tir_valor = resultados['tir']

            # Solo override si la respuesta del AI es MUY corta (menos de 50 caracteres)
            # Esto indica que el AI dio una respuesta demasiado breve y necesitamos contextualizar
            is_too_short_response = len(respuesta.strip()) < 50

            # Y el usuario est√° preguntando espec√≠ficamente sobre interpretaci√≥n del resultado
            user_asking_interpretation = any(word in mensaje_usuario.lower() for word in [
                'qu√© significa', 'que significa', 'interpretar', 'como interpretar',
                'esta tir', 'la tir', 'mi tir', 'tir calculada', 'significa'
            ])

            if is_too_short_response and user_asking_interpretation:
                logger.info(f"üéØ SELECTIVE OVERRIDE: AI response too short ({len(respuesta)} chars), providing contextual interpretation for TIR={tir_valor}%")

                # Respuesta contextual completa en texto plano con marcadores simples
                respuesta_contextual = f"""**Interpretaci√≥n de tu TIR del [blue]{tir_valor}%[/blue]**

Bas√°ndome en tu c√°lculo de **[blue]TIR[/blue]** del **[blue]{tir_valor}%[/blue]**, te explico qu√© significa este resultado.

Este **[blue]{tir_valor}%[/blue]** representa el rendimiento real anual que genera tu inversi√≥n, considerando todos los flujos de caja que proyectaste y el tiempo en que ocurren."""

                # Evaluaci√≥n contextual detallada
                if tir_valor >= 20:
                    respuesta_contextual += f"""

Una **[blue]TIR[/blue]** del **[green]{tir_valor}%[/green]** se considera **[green]excelente[/green]** y supera ampliamente el costo de capital promedio en Per√∫ (alrededor del 12-15%). Tu proyecto tiene un rendimiento excepcional."""
                elif tir_valor >= 15:
                    respuesta_contextual += f"""

Una **[blue]TIR[/blue]** del **[green]{tir_valor}%[/green]** se considera **[green]muy buena[/green]** y est√° por encima del costo de capital promedio. Es un resultado s√≥lido."""
                elif tir_valor >= 12:
                    respuesta_contextual += f"""

Una **[blue]TIR[/blue]** del **[blue]{tir_valor}%[/blue]** se considera **[blue]aceptable[/blue]**, comparable con el costo de capital en Per√∫. Es un resultado razonable."""
                elif tir_valor >= 8:
                    respuesta_contextual += f"""

Una **[blue]TIR[/blue]** del **[orange]{tir_valor}%[/orange]** se considera **[orange]baja[/orange]** y est√° por debajo del costo de capital t√≠pico. Merece evaluaci√≥n adicional."""
                else:
                    respuesta_contextual += f"""

Una **[blue]TIR[/blue]** del **[red]{tir_valor}%[/red]** se considera **[red]muy baja[/red]** y sugiere que el proyecto podr√≠a no ser rentable con los par√°metros actuales."""

                respuesta_contextual += f"""

**üí° ¬øQu√© significa esto para tu proyecto?**
‚Ä¢ Si tu costo de capital es menor al **[blue]{tir_valor}%[/blue]**, la inversi√≥n es **[green]rentable[/green]**
‚Ä¢ El **[blue]{tir_valor}%[/blue]** representa el rendimiento real de tu proyecto
‚Ä¢ Compara este **[blue]{tir_valor}%[/blue]** con otras oportunidades de inversi√≥n

¬øTe gustar√≠a que te ayude a identificar oportunidades para mejorar este resultado, o tienes alguna duda espec√≠fica sobre c√≥mo interpretar este **[blue]{tir_valor}%[/blue]** en el contexto de tu proyecto?

[¬øC√≥mo mejorar esta TIR?|¬øEs rentable mi proyecto?|¬øQu√© factores afectan la TIR?]"""

                return respuesta_contextual
            else:
                logger.info(f"‚úÖ AI response is adequate ({len(respuesta)} chars) or user not asking for interpretation - no override needed")

        return respuesta

    def _override_respuestas_van(self, respuesta: str, analysis_context: Dict, mensaje_usuario: str) -> str:
        """
        Override selectivo para preguntas sobre VAN
        """
        if not analysis_context or not isinstance(analysis_context, dict):
            return respuesta

        tipo_analisis = analysis_context.get('tipo_analisis')
        resultados = analysis_context.get('resultados', {})

        if tipo_analisis == 'van' and resultados.get('van') is not None:
            van_valor = resultados['van']

            is_too_short_response = len(respuesta.strip()) < 50
            user_asking_about_van = any(word in mensaje_usuario.lower() for word in [
                'van', 'valor actual', 'qu√© significa', 'que significa', 'interpretar', 'como interpretar'
            ])

            if is_too_short_response and user_asking_about_van:
                logger.info(f"üéØ OVERRIDE VAN: Providing contextual interpretation for VAN={van_valor}")

                respuesta_contextual = f"""**Interpretaci√≥n de tu VAN de S/ {van_valor:,.2f}**

Bas√°ndome en tu c√°lculo de **[blue]VAN[/blue]** de **S/ {van_valor:,.2f}**, te explico qu√© significa este resultado.

Este **[blue]VAN[/blue]** representa el beneficio neto actualizado de tu proyecto, considerando todos los flujos de caja descontados a valor presente."""

                if van_valor > 0:
                    respuesta_contextual += f"""

Un **[blue]VAN[/blue]** **[green]positivo[/green]** de **S/ {van_valor:,.2f}** indica que tu proyecto es **[green]rentable[/green]** y generar√° un beneficio neto superior a la inversi√≥n inicial."""
                elif van_valor < 0:
                    respuesta_contextual += f"""

Un **[blue]VAN[/blue]** **[red]negativo[/red]** de **S/ {van_valor:,.2f}** indica que tu proyecto **[red]no es rentable[/red]** y destruir√° valor."""
                else:
                    respuesta_contextual += f"""

Un **[blue]VAN[/blue]** de **S/ {van_valor:,.2f}** indica el **[orange]punto de equilibrio[/orange]** donde el proyecto ni gana ni pierde valor."""

                respuesta_contextual += f"""

**üí° ¬øQu√© significa esto para tu proyecto?**
‚Ä¢ **[blue]VAN > 0[/blue]**: Proyecto **[green]viable financieramente[/green]**
‚Ä¢ **[blue]VAN < 0[/blue]**: Proyecto **[red]no viable[/red]**, requiere revisi√≥n
‚Ä¢ **[blue]VAN = 0[/blue]**: Punto de equilibrio, decisi√≥n depende de otros factores

¬øTe gustar√≠a explorar escenarios alternativos para mejorar este VAN, o tienes alguna duda espec√≠fica sobre su interpretaci√≥n?

[¬øC√≥mo mejorar el VAN?|¬øQu√© factores afectan el VAN?|¬øEs rentable mi proyecto?]"""

                return respuesta_contextual

        return respuesta

    def _override_respuestas_wacc(self, respuesta: str, analysis_context: Dict, mensaje_usuario: str) -> str:
        """
        Override selectivo para preguntas sobre WACC
        """
        if not analysis_context or not isinstance(analysis_context, dict):
            return respuesta

        tipo_analisis = analysis_context.get('tipo_analisis')
        resultados = analysis_context.get('resultados', {})

        if tipo_analisis == 'wacc' and resultados.get('wacc') is not None:
            wacc_valor = resultados['wacc']

            is_too_short_response = len(respuesta.strip()) < 50
            user_asking_about_wacc = any(word in mensaje_usuario.lower() for word in [
                'wacc', 'costo capital', 'qu√© significa', 'que significa', 'interpretar', 'como interpretar'
            ])

            if is_too_short_response and user_asking_about_wacc:
                logger.info(f"üéØ OVERRIDE WACC: Providing contextual interpretation for WACC={wacc_valor}%")

                respuesta_contextual = f"""**Interpretaci√≥n de tu WACC del [red]{wacc_valor}%[/red]**

Tu **[red]WACC[/red]** calculado es del **[red]{wacc_valor}%[/red]**, que representa el costo promedio ponderado de tu capital.

Este **[red]WACC[/red]** es la tasa m√≠nima de retorno que deben generar tus proyectos para crear valor para los inversionistas."""

                if wacc_valor < 12:
                    respuesta_contextual += f"""

Un **[red]WACC[/red]** del **[green]{wacc_valor}%[/green]** se considera **[green]relativamente bajo[/green]**, lo que facilita la rentabilidad de proyectos."""
                elif wacc_valor < 15:
                    respuesta_contextual += f"""

Un **[red]WACC[/red]** del **[blue]{wacc_valor}%[/blue]** est√° en el **[blue]rango promedio[/blue]** del mercado peruano."""
                else:
                    respuesta_contextual += f"""

Un **[red]WACC[/red]** del **[orange]{wacc_valor}%[/orange]** se considera **[orange]elevado[/orange]**, lo que hace m√°s dif√≠cil la rentabilidad de proyectos."""

                respuesta_contextual += f"""

**üí° ¬øQu√© significa esto para tu empresa?**
‚Ä¢ Proyectos con **[blue]TIR > {wacc_valor}%[/blue]** son candidatos viables
‚Ä¢ El **[red]WACC[/red]** es tu "tasa de descuento" para calcular VAN
‚Ä¢ Un **[red]WACC[/red]** m√°s bajo mejora las oportunidades de inversi√≥n

¬øTe gustar√≠a explorar estrategias para reducir tu WACC o analizar c√≥mo usarlo en evaluaciones de proyectos?

[¬øC√≥mo reducir el WACC?|¬øC√≥mo usar este WACC?|¬øEs alto o bajo este costo de capital?]"""

                return respuesta_contextual

        return respuesta

    def _override_respuestas_portafolio(self, respuesta: str, analysis_context: Dict, mensaje_usuario: str) -> str:
        """
        Override selectivo para preguntas sobre portafolio
        """
        if not analysis_context or not isinstance(analysis_context, dict):
            return respuesta

        tipo_analisis = analysis_context.get('tipo_analisis')
        resultados = analysis_context.get('resultados', {})

        if tipo_analisis == 'portafolio' and resultados.get('retorno') is not None:
            retorno = resultados.get('retorno', 0)
            riesgo = resultados.get('riesgo', 0)
            sharpe = resultados.get('sharpe', 0)

            is_too_short_response = len(respuesta.strip()) < 50
            user_asking_about_portafolio = any(word in mensaje_usuario.lower() for word in [
                'portafolio', 'riesgo', 'retorno', 'sharpe', 'qu√© significa', 'que significa', 'interpretar'
            ])

            if is_too_short_response and user_asking_about_portafolio:
                logger.info(f"üéØ OVERRIDE PORTAFOLIO: Providing contextual interpretation for Portfolio")

                respuesta_contextual = f"""**Interpretaci√≥n de tu Portafolio Optimizado**

Tu portafolio tiene un retorno esperado del **[green]{retorno}%[/green]** con un riesgo del **[orange]{riesgo}%[/orange]** (desviaci√≥n est√°ndar).

El **[blue]Ratio Sharpe[/blue]** calculado es **[blue]{sharpe:.2f}[/blue]**, que mide la eficiencia riesgo-retorno de tu inversi√≥n."""

                if sharpe > 1:
                    respuesta_contextual += f"""

Tu portafolio tiene una **[green]excelente eficiencia[/green]** con un Ratio Sharpe superior a 1."""
                elif sharpe > 0.5:
                    respuesta_contextual += f"""

Tu portafolio tiene una **[blue]buena eficiencia[/blue]** riesgo-retorno."""
                else:
                    respuesta_contextual += f"""

Tu portafolio requiere **[orange]optimizaci√≥n[/orange]** para mejorar la relaci√≥n riesgo-retorno."""

                respuesta_contextual += f"""

**üí° An√°lisis de tu portafolio:**
‚Ä¢ **[green]Retorno esperado[/green]**: {retorno}% anual
‚Ä¢ **[orange]Riesgo (volatilidad)[/orange]**: {riesgo}% anual  
‚Ä¢ **[blue]Ratio Sharpe[/blue]**: {sharpe:.2f} (eficiencia)

¬øTe gustar√≠a explorar estrategias de diversificaci√≥n adicionales o analizar escenarios de mercado alternativos?

[¬øC√≥mo diversificar mejor?|¬øQu√© recomendaciones tienes?|¬øCu√°l es el riesgo √≥ptimo?]"""

                return respuesta_contextual

        return respuesta

    def _override_respuestas_ml(self, respuesta: str, analysis_context: Dict, mensaje_usuario: str) -> str:
        """
        Override selectivo para preguntas sobre an√°lisis ML
        """
        if not analysis_context or not isinstance(analysis_context, dict):
            return respuesta

        tipo_analisis = analysis_context.get('tipo_analisis')
        resultados = analysis_context.get('resultados', {})

        if tipo_analisis in ['prediccion', 'montecarlo', 'tornado', 'sensibilidad']:
            is_too_short_response = len(respuesta.strip()) < 50
            user_asking_about_ml = any(word in mensaje_usuario.lower() for word in [
                'predicciones', 'montecarlo', 'tornado', 'sensibilidad', 'an√°lisis', 'resultados',
                'qu√© significa', 'que significa', 'interpretar', 'como interpretar'
            ])

            if is_too_short_response and user_asking_about_ml:
                logger.info(f"üéØ OVERRIDE ML: Providing contextual interpretation for {tipo_analisis}")

                if tipo_analisis == 'prediccion':
                    precision = resultados.get('precision', 0)
                    tendencia = resultados.get('tendencia_principal', 'tendencias mixtas')

                    respuesta_contextual = f"""**Interpretaci√≥n de tus Predicciones ML**

El modelo de Machine Learning gener√≥ predicciones con una **[blue]precisi√≥n del {precision}%[/blue]**.

Las tendencias identificadas muestran **{tendencia}** para los pr√≥ximos per√≠odos."""

                    if precision > 85:
                        respuesta_contextual += f"""

La **[green]alta precisi√≥n[/green]** del modelo hace que estas predicciones sean **[green]muy confiables[/green]** para la toma de decisiones."""
                    elif precision > 70:
                        respuesta_contextual += f"""

La **[blue]buena precisi√≥n[/blue]** del modelo proporciona informaci√≥n **[blue]√∫til[/blue]** como referencia."""
                    else:
                        respuesta_contextual += f"""

La precisi√≥n limitada sugiere usar estas predicciones con **[orange]precauci√≥n[/orange]** y combinarlas con otros an√°lisis."""

                    respuesta_contextual += f"""

¬øTe gustar√≠a explorar estrategias basadas en estas predicciones o profundizar en alg√∫n aspecto espec√≠fico del an√°lisis?

[¬øC√≥mo usar estas predicciones?|¬øQu√© riesgos debo considerar?|¬øQu√© estrategias recomiendas?]"""

                elif tipo_analisis == 'montecarlo':
                    van_promedio = resultados.get('van_promedio', 0)
                    probabilidad_positivo = resultados.get('probabilidad_positivo', 0)

                    respuesta_contextual = f"""**Interpretaci√≥n de tu Simulaci√≥n Monte Carlo**

La simulaci√≥n gener√≥ un **[blue]VAN promedio[/blue]** de **S/ {van_promedio:,.2f}** con una **[blue]probabilidad de VAN positivo[/blue]** del **[green]{probabilidad_positivo}%[/green]**."""

                    if probabilidad_positivo > 80:
                        respuesta_contextual += f"""

La **[green]alta probabilidad de √©xito[/green]** indica un proyecto **[green]muy robusto[/green]** ante la incertidumbre."""
                    elif probabilidad_positivo > 60:
                        respuesta_contextual += f"""

La **[blue]probabilidad aceptable[/blue]** sugiere que el proyecto es **[blue]viable[/blue]** pero requiere gesti√≥n de riesgos."""
                    else:
                        respuesta_contextual += f"""

La **[red]baja probabilidad[/red]** indica necesidad de **[orange]revisar los par√°metros[/orange]** del proyecto."""

                    respuesta_contextual += f"""

¬øTe gustar√≠a analizar escenarios espec√≠ficos o explorar estrategias de mitigaci√≥n de riesgos?

[¬øC√≥mo interpretar estos resultados?|¬øQu√© riesgos debo considerar?|¬øQu√© estrategias recomiendas?]"""

                elif tipo_analisis == 'tornado':
                    variable_mas_sensible = resultados.get('variable_mas_sensible', 'N/A')
                    impacto_maximo = resultados.get('impacto_maximo', 0)

                    respuesta_contextual = f"""**Interpretaci√≥n de tu An√°lisis Tornado**

La variable m√°s sensible identificada es **"{variable_mas_sensible}"** con un **[red]impacto m√°ximo[/red]** del **[red]{impacto_maximo}%[/red]** en el VAN."""

                    if impacto_maximo > 50:
                        respuesta_contextual += f"""

Este **[red]alto impacto[/red]** indica que **"{variable_mas_sensible}"** puede hacer **[red]inviable[/red]** el proyecto si cambia desfavorablemente."""
                    elif impacto_maximo > 25:
                        respuesta_contextual += f"""

El **[orange]impacto moderado[/orange]** sugiere monitorear de cerca **"{variable_mas_sensible}"**."""
                    else:
                        respuesta_contextual += f"""

El **[green]impacto limitado[/green]** de **"{variable_mas_sensible}"** indica **[green]baja sensibilidad[/green]** del proyecto."""

                    respuesta_contextual += f"""

¬øTe gustar√≠a explorar estrategias de mitigaci√≥n para las variables cr√≠ticas identificadas?

[¬øC√≥mo reducir la sensibilidad?|¬øQu√© planes de contingencia recomiendas?|¬øC√≥mo monitorear estas variables?]"""

                elif tipo_analisis == 'sensibilidad':
                    punto_equilibrio = resultados.get('punto_equilibrio', 0)
                    elasticidad_critica = resultados.get('elasticidad_critica', 0)

                    respuesta_contextual = f"""**Interpretaci√≥n de tu An√°lisis de Sensibilidad**

El **[blue]punto de equilibrio[/blue]** identificado es de **[blue]{punto_equilibrio} unidades[/blue]**, y la **[red]elasticidad cr√≠tica[/red]** es **[red]{elasticidad_critica:.2f}[/red]**."""

                    if abs(elasticidad_critica) > 2:
                        respuesta_contextual += f"""

La **[red]alta elasticidad[/red]** indica que peque√±os cambios tienen **[red]gran impacto[/red]** en los resultados."""
                    elif abs(elasticidad_critica) > 1:
                        respuesta_contextual += f"""

La **[orange]elasticidad moderada[/orange]** sugiere **[orange]sensibilidad media[/orange]** a cambios."""
                    else:
                        respuesta_contextual += f"""

La **[green]baja elasticidad[/green]** indica **[green]estabilidad[/green]** ante variaciones."""

                    respuesta_contextual += f"""

¬øTe gustar√≠a explorar escenarios "qu√© pasar√≠a si" basados en este an√°lisis?

[¬øC√≥mo optimizar el punto de equilibrio?|¬øQu√© estrategias de mitigaci√≥n?|¬øC√≥mo mejorar la estabilidad?]"""

                return respuesta_contextual

        return respuesta

    def _aplicar_formato_automatico(self, respuesta: str, contexto: Dict, analysis_context: Dict, nivel: str) -> str:
        """
        Aplica formato autom√°tico de colores y preguntas sugeridas a las respuestas de IA
        """
        try:
            # Aplicar colores autom√°ticos a palabras clave
            respuesta = self._aplicar_colores_automaticos(respuesta, contexto, analysis_context)

            # Agregar preguntas sugeridas autom√°ticas
            respuesta = self._agregar_preguntas_sugeridas(respuesta, contexto, analysis_context, nivel)

            return respuesta
        except Exception as e:
            logger.error(f"Error aplicando formato autom√°tico: {e}")
            return respuesta

    def _aplicar_colores_automaticos(self, respuesta: str, contexto: Dict, analysis_context: Dict) -> str:
        """
        Aplica colores autom√°ticos a palabras clave importantes en la respuesta
        """
        import re

        # Colores para resultados positivos
        respuesta = re.sub(r'\bpositivo\b', '<strong style="color: #059669;">positivo</strong>', respuesta, flags=re.IGNORECASE)
        respuesta = re.sub(r'\brentable\b', '<strong style="color: #059669;">rentable</strong>', respuesta, flags=re.IGNORECASE)
        respuesta = re.sub(r'\bbeneficio\b', '<strong style="color: #059669;">beneficio</strong>', respuesta, flags=re.IGNORECASE)
        respuesta = re.sub(r'\bganancia\b', '<strong style="color: #059669;">ganancia</strong>', respuesta, flags=re.IGNORECASE)

        # Colores para resultados negativos
        respuesta = re.sub(r'\bnegativo\b', '<strong style="color: #dc2626;">negativo</strong>', respuesta, flags=re.IGNORECASE)
        respuesta = re.sub(r'\bp√©rdida\b', '<strong style="color: #dc2626;">p√©rdida</strong>', respuesta, flags=re.IGNORECASE)
        respuesta = re.sub(r'\briesgo\b', '<strong style="color: #dc2626;">riesgo</strong>', respuesta, flags=re.IGNORECASE)
        respuesta = re.sub(r'\bpeligro\b', '<strong style="color: #dc2626;">peligro</strong>', respuesta, flags=re.IGNORECASE)

        # Colores para conceptos importantes
        respuesta = re.sub(r'\bVAN\b', '<strong style="color: #2563eb;">VAN</strong>', respuesta)
        respuesta = re.sub(r'\bTIR\b', '<strong style="color: #7c3aed;">TIR</strong>', respuesta)
        respuesta = re.sub(r'\bWACC\b', '<strong style="color: #dc2626;">WACC</strong>', respuesta)

        # Colores para advertencias
        respuesta = re.sub(r'\badvertencia\b', '<strong style="color: #ea580c;">advertencia</strong>', respuesta, flags=re.IGNORECASE)
        respuesta = re.sub(r'\bconsidera\b', '<strong style="color: #ea580c;">considera</strong>', respuesta, flags=re.IGNORECASE)
        respuesta = re.sub(r'\brecomienda\b', '<strong style="color: #ea580c;">recomienda</strong>', respuesta, flags=re.IGNORECASE)

        # Colores para valores monetarios
        respuesta = re.sub(r'(S/\s*\d{1,3}(?:\.\d{3})*(?:,\d{2})?)', r'<strong style="color: #059669;">\1</strong>', respuesta)

        return respuesta

    def _agregar_preguntas_sugeridas(self, respuesta: str, contexto: Dict, analysis_context: Dict, nivel: str) -> str:
        """
        Agrega preguntas sugeridas autom√°ticas basadas en el contexto
        """
        preguntas_sugeridas = []

        # Determinar tipo de an√°lisis o contexto
        tipo_analisis = None
        if analysis_context and isinstance(analysis_context, dict):
            tipo_analisis = analysis_context.get('tipo_analisis')
        elif contexto and isinstance(contexto, dict):
            tipo_analisis = contexto.get('type')

        # Preguntas basadas en el tipo de an√°lisis
        if tipo_analisis == 'van':
            preguntas_sugeridas = [
                "¬øC√≥mo mejorar el VAN de mi proyecto?",
                "¬øQu√© factores afectan m√°s el VAN?",
                "¬øEs rentable mi inversi√≥n?"
            ]
        elif tipo_analisis == 'tir':
            preguntas_sugeridas = [
                "¬øQu√© significa esta TIR?",
                "¬øC√≥mo comparar con otras inversiones?",
                "¬øEs buena esta tasa de retorno?"
            ]
        elif tipo_analisis == 'wacc':
            preguntas_sugeridas = [
                "¬øC√≥mo usar este WACC en mis c√°lculos?",
                "¬øEs alto o bajo este costo de capital?",
                "¬øC√≥mo reducir mi WACC?"
            ]
        elif tipo_analisis == 'portafolio':
            preguntas_sugeridas = [
                "¬øC√≥mo diversificar mejor mi portafolio?",
                "¬øCu√°l es el riesgo de mis inversiones?",
                "¬øQu√© recomendaciones tienes para optimizar?"
            ]
        elif tipo_analisis in ['prediccion', 'montecarlo', 'tornado', 'escenarios']:
            preguntas_sugeridas = [
                "¬øC√≥mo interpretar estos resultados?",
                "¬øQu√© riesgos debo considerar?",
                "¬øQu√© estrategias recomiendas?"
            ]
        else:
            # Solo usar detecci√≥n basada en contenido si no hay tipo_analisis espec√≠fico
            # Priorizar el tipo de an√°lisis sobre el contenido de la respuesta
            if 'van' in respuesta.lower() and not tipo_analisis:
                preguntas_sugeridas = [
                    "¬øQu√© es el VAN?",
                    "¬øC√≥mo calcular el VAN?",
                    "¬øC√≥mo interpretar el VAN?"
                ]
            elif 'tir' in respuesta.lower() and not tipo_analisis:
                preguntas_sugeridas = [
                    "¬øQu√© es la TIR?",
                    "¬øC√≥mo calcular la TIR?",
                    "¬øC√≥mo usar la TIR?"
                ]
            elif 'wacc' in respuesta.lower() and not tipo_analisis:
                preguntas_sugeridas = [
                    "¬øQu√© es el WACC?",
                    "¬øC√≥mo calcular el WACC?",
                    "¬øPara qu√© usar el WACC?"
                ]
            else:
                preguntas_sugeridas = [
                    "¬øPuedes explicarme mejor?",
                    "¬øTienes un ejemplo pr√°ctico?",
                    "¬øCu√°les son las limitaciones?"
                ]

        # Agregar preguntas sugeridas al final de la respuesta
        if preguntas_sugeridas:
            respuesta += f"\n\n[{'|'.join(preguntas_sugeridas)}]"

        return respuesta

    def _log_conversacion(self, usuario_id: Optional[int], mensaje_usuario: str,
                         respuesta_ia: str, proveedor: str, contexto: Dict, nivel: str, analysis_context: Dict = None):
        """
        Registra la conversaci√≥n en la base de datos
        """
        try:
            db = get_db_connection()
            cursor = db.cur

            # Crear tabla si no existe
            cursor.execute("""
                CREATE TABLE IF NOT EXISTS Conversaciones_Chatbot (
                    conversacion_id INTEGER PRIMARY KEY AUTOINCREMENT,
                    usuario_id INTEGER,
                    mensaje_usuario TEXT NOT NULL,
                    respuesta_ia TEXT NOT NULL,
                    proveedor_ia VARCHAR(20) DEFAULT 'groq',
                    nivel_usuario VARCHAR(20) DEFAULT 'basico',
                    contexto TEXT,
                    tipo_interaccion VARCHAR(50),
                    fecha TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                    FOREIGN KEY (usuario_id) REFERENCES Usuarios(usuario_id)
                )
            """)

            # Determinar tipo de interacci√≥n
            tipo_interaccion = "consulta_general"
            if contexto and isinstance(contexto, dict):
                sim_type = contexto.get('type')
                if sim_type:
                    tipo_interaccion = f"simulacion_{sim_type.lower()}"
                elif any(word in mensaje_usuario.lower() for word in ["van", "tir", "wacc", "valor", "tasa", "retorno"]):
                    tipo_interaccion = "consulta_tecnica"

            # Insertar conversaci√≥n
            cursor.execute("""
                INSERT INTO Conversaciones_Chatbot
                (usuario_id, mensaje_usuario, respuesta_ia, proveedor_ia, nivel_usuario, contexto, tipo_interaccion)
                VALUES (?, ?, ?, ?, ?, ?, ?)
            """, (
                usuario_id,
                mensaje_usuario,
                respuesta_ia,
                proveedor,
                nivel,
                json.dumps(contexto) if contexto else None,
                tipo_interaccion
            ))

            db.commit()

            logger.info(f"‚úÖ Conversaci√≥n logged - Usuario: {usuario_id}, Proveedor: {proveedor}, Nivel: {nivel}")

        except Exception as e:
            logger.error(f"‚ùå Error logging conversaci√≥n: {e}")

    def obtener_estadisticas_conversaciones(self, usuario_id: Optional[int] = None) -> Dict[str, Any]:
        """
        Obtiene estad√≠sticas de conversaciones para an√°lisis
        """
        try:
            db = get_db_connection()
            cursor = db.cur

            if usuario_id:
                # Estad√≠sticas por usuario
                cursor.execute("""
                    SELECT
                        COUNT(*) as total_conversaciones,
                        COUNT(DISTINCT DATE(fecha)) as dias_activos,
                        AVG(LENGTH(mensaje_usuario)) as longitud_promedio_mensajes,
                        proveedor_ia,
                        COUNT(*) as uso_por_proveedor
                    FROM Conversaciones_Chatbot
                    WHERE usuario_id = ?
                    GROUP BY proveedor_ia
                """, (usuario_id,))

                stats_proveedores = cursor.fetchall()

                cursor.execute("""
                    SELECT tipo_interaccion, COUNT(*) as cantidad
                    FROM Conversaciones_Chatbot
                    WHERE usuario_id = ?
                    GROUP BY tipo_interaccion
                    ORDER BY cantidad DESC
                """, (usuario_id,))

                tipos_interaccion = cursor.fetchall()

            else:
                # Estad√≠sticas globales
                cursor.execute("""
                    SELECT COUNT(*) as total_conversaciones,
                           COUNT(DISTINCT usuario_id) as usuarios_unicos,
                           AVG(LENGTH(mensaje_usuario)) as longitud_promedio
                    FROM Conversaciones_Chatbot
                """)

                stats_globales = cursor.fetchone()
                stats_proveedores = []
                tipos_interaccion = []

            return {
                "estadisticas_proveedores": stats_proveedores,
                "tipos_interaccion": tipos_interaccion,
                "fecha_generacion": datetime.now().isoformat()
            }

        except Exception as e:
            logger.error(f"Error obteniendo estad√≠sticas: {e}")
            return {"error": str(e)}

# Instancia global del servicio
chatbot_servicio = ChatbotServicio()

def obtener_servicio_chatbot() -> ChatbotServicio:
    """Factory function para obtener instancia del servicio"""
    return chatbot_servicio
